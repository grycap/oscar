{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>OSCAR is an open-source platform to support the event-driven serverless computing model for data-processing applications. It can be automatically deployed on multi-Clouds, and even on low-powered devices, to create highly-parallel event-driven data-processing serverless applications along the computing continuum. These applications execute on customized runtime environments provided by Docker containers that run on elastic Kubernetes clusters. It is also integrated with the  SCAR framework, which supports a High Throughput Computing Programming Model to create highly-parallel event-driven data-processing serverless applications that execute on customized runtime environments provided by Docker containers run on AWS Lambda and AWS Batch.</p>"},{"location":"#concepts","title":"Concepts","text":"<ul> <li>OSCAR Cluster: A Kubernetes cluster (either fixed size or elastic) configured with the OSCAR services and components. The cluster must have at least one front-end node, which executes the OSCAR control plane and one or several working nodes.</li> <li>OSCAR Service: The execution unit in the OSCAR framework, typically defined in FDL, by a:<ul> <li>Docker image, providing the customized runtime environment for an application.</li> <li>Execution requirements.</li> <li>User-defined shell script that will be executed in a dynamically-provisioned container.</li> <li>(Optional) The object storage that will trigger the execution of the OSCAR service upon a file upload. </li> <li>(Optional) The object storage(s) on which the output results of the OSCAR service will be stored. </li> <li>(Optional) Deployment strategy and additional configuration. </li> </ul> </li> </ul>"},{"location":"#rationale","title":"Rationale","text":"<p>Users create OSCAR services to:</p> <ul> <li>Execute a containerized command-line application or web service in response to:<ul> <li>a file upload to an object store (e.g. MinIO), thus supporting loosely-coupled High-Throughput Computing use cases where many files need to be processed in parallel in a distributed computing platform.</li> <li>a request to a load-balanced auto-scaled HTTP-based endpoints, thus allowing to exposed generic scientific applications as highly-scalable HTTP endpoints.</li> </ul> </li> <li>Execute a pipeline of multiple OSCAR service where the output data of one triggers the execution of another OSCAR service, potentially running in different clusters, thus creating event-driven scalable pipelines along the computing continuum.</li> </ul> <p>An admin user can deploy an OSCAR cluster on a Cloud platform so that other users belonging to a Virtual Organization (VO) can create OSCAR services. A VO is a group of people (e.g. scientists, researchers) with common interests and requirements, who need to work collaboratively and/or share resources (e.g. data, software, expertise, CPU, storage space) regardless of geographical location. OSCAR supports the VOs defined in EGI, which are listed in the 'Operations Portal'. EGI is the European's largest federation of computing and storage resource providers united by a mission of delivering advanced computing and data analytics services for research and innovation.</p>"},{"location":"#architecture-components","title":"Architecture &amp; Components","text":"<p>OSCAR runs on an elastic Kubernetes cluster that is deployed using:</p> <ul> <li>IM, an open-source virtual infrastructure     provisioning tool for multi-Clouds.</li> </ul> <p>The following components are deployed inside the Kubernetes cluster in order to support the OSCAR platform:</p> <ul> <li>CLUES, an elasticity manager that     horizontally scales in and out the number of nodes of the Kubernetes     cluster according to the workload.</li> <li>MinIO, a high-performance distributed object storage     server that provides an API compatible with S3.</li> <li>Knative, a serverless framework to serve     container-based applications for synchronous invocations (default     Serverless Backend).</li> <li>OSCAR Manager, the main API, responsible for the management of the services and the integration of the different components. </li> <li>OSCAR Dashboard, an easy-to-use web-based graphical user interface aimed at end users.</li> </ul> <p>As external storage providers, the following services can be used:</p> <ul> <li>External MinIO servers, which may be in clusters other     than the platform.</li> <li>Amazon S3, AWS's object storage     service that offers industry-leading scalability, data availability,     security, and performance in the public Cloud.</li> <li>Onedata, the global data access solution for science     used in the EGI Federated Cloud.</li> <li>Any storage provider that can be accessible through     WebDAV protocol. An example of a storage provider     supporting this protocol is dCache, a storage     middleware system capable of managing the storage and exchange of large data     quantities.</li> </ul> <p>Note: All of the mentioned storage providers can be used as output, but only MinIO can be used as input. Amazon S3 and dCache  need DCNiOS as output.</p> <p>An OSCAR cluster can be easily deployed via the IM Dashboard on any major public and on-premises Cloud provider, including the EGI Federated Cloud.</p> <p>A summary of the components used: </p> <p></p> <p>An OSCAR cluster can be accessed via its REST API, the web-based  OSCAR Dashboard and the command-line interface provided by OSCAR CLI.</p>"},{"location":"about/","title":"Acknowledgements","text":"<p>OSCAR has been developed by the Grid and High Performance Computing Group (GRyCAP) at the Instituto de Instrumentaci\u00f3n para Imagen Molecular (I3M) from the Universitat Polit\u00e8cnica de Val\u00e8ncia (UPV).</p> <p> </p> <p>OSCAR has been supported by the following projects:</p> <ul> <li>EGI Strategic and Innovation Fund. </li> <li>OSCARISER (Open Serverless Computing for the Adoption of Rapid Innovation on Secure Enterprise-ready Resources). Project PDC2021-120844-I00 funded by MICIU/AEI/10.13039/501100011033 and by the European Union NextGenerationEU/PRTR</li> <li>SERCLOCO (Serverless Scientific Computing Across the Hybrid Cloud Continuum). Grant PID2020-113126RB-I00 funded by MICIU/AEI/10.13039/501100011033.</li> <li>AI-SPRINT (AI in Secure Privacy-Preserving Computing Continuum) that has received funding from the European Union\u2019s Horizon 2020 Research and Innovation Programme under Grant 101016577.</li> <li>AI4EOSC (Artificial Intelligence for the European Open Science Cloud) that has received funding from the European Union\u2019s Horizon Europe Research and Innovation Programme under Grant 101058593.</li> <li>interTwin (An interdisciplinary Digital Twin Engine for science)  that has received funding from the European Union\u2019s Horizon Europe Programme under Grant 101058386.</li> </ul>"},{"location":"about/#contact","title":"Contact","text":"<p>If you have any trouble please open an issue or email us.</p>"},{"location":"additional-config/","title":"Additional configuration","text":"<p>To give the administrator a more personalized cluster configuration, the OSCAR manager searches for a config map on the cluster with the additional properties to apply. Since this is still a work in progress, the only configurable property currently is the container images' origin. As seen in the following ConfigMap definition, you can set a list of \"prefixes\" that you consider secure repositories, so images that do not come from one of these are restricted.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: additional-oscar-config\n  namespace: oscar-svc\ndata:\n  config.yaml: |\n    images:\n      allowed_prefixes:\n      - ghcr.io\n</code></pre> <p>Additionally, this property can be added when creating an OSCAR cluster through the IM, which will automatically create the ConfigMap.</p> <p></p>"},{"location":"api/","title":"OSCAR API","text":"<p>OSCAR exposes a secure REST API available at the Kubernetes master's node IP through an Ingress Controller. This API has been described following the OpenAPI Specification and it is available below.</p> <p>\u2139\ufe0f</p> <p>The bearer token used to run a service can be either the OSCAR service access token or the user's Access Token if the OSCAR cluster is integrated with EGI Check-in.</p> <p></p>"},{"location":"deploy-ec3/","title":"Deployment with EC3","text":"<p>\u2757\ufe0f The deployment of OSCAR with EC3 is deprecated. Please, consider using the IM Dashboard.</p> <p>In order to deploy an elastic Kubernetes cluster with the OSCAR platform, it is preferable to use the IM Dashboard.  Alternatively, you can also use EC3, a tool that deploys elastic virtual clusters. EC3 uses the Infrastructure Manager (IM) to deploy such clusters on multiple Cloud back-ends. The installation details can be found here, though this section includes the relevant information to get you started.</p>"},{"location":"deploy-ec3/#prepare-ec3","title":"Prepare EC3","text":"<p>Clone the EC3 repository:</p> <pre><code>git clone https://github.com/grycap/ec3\n</code></pre> <p>Download the OSCAR template into the <code>ec3/templates</code> folder:</p> <pre><code>cd ec3\nwget -P templates https://raw.githubusercontent.com/grycap/oscar/master/templates/oscar.radl\n</code></pre> <p>Create an <code>auth.txt</code> authorization file with valid credentials to access your Cloud provider. As an example, to deploy on an OpenNebula-based Cloud site the contents of the file would be:</p> <pre><code>type = OpenNebula; host = opennebula-host:2633;\nusername = your-user;\npassword = you-password\n</code></pre> <p>Modify the corresponding RADL template in order to determine the appropriate configuration for your deployment:</p> <ul> <li>Virtual Machine Image identifiers</li> <li>Hardware Configuration</li> </ul> <p>As an example, to deploy in OpenNebula, one would modify the <code>ubuntu-opennebula.radl</code> (or create a new one).</p>"},{"location":"deploy-ec3/#deploy-the-cluster","title":"Deploy the cluster","text":"<p>To deploy the cluster, execute:</p> <pre><code>./ec3 launch oscar-cluster oscar ubuntu-opennebula -a auth.txt\n</code></pre> <p>This will take several minutes until the Kubernetes cluster and all the required services have been deployed. You will obtain the IP of the front-end of the cluster and a confirmation message that the front-end is ready. Notice that it will still take few minutes before the services in the Kubernetes cluster are up &amp; running.</p>"},{"location":"deploy-ec3/#check-the-cluster-state","title":"Check the cluster state","text":"<p>The cluster will be fully configured when all the Kubernetes pods are in the <code>Running</code> state.</p> <pre><code> ./ec3 ssh oscar-cluster\n sudo kubectl get pods --all-namespaces\n</code></pre> <p>Notice that initially only the front-end node of the cluster is deployed. As soon as the OSCAR framework is deployed, together with its services, the CLUES elasticity manager powers on a new (working) node on which these services will be run.</p> <p>You can see the status of the provisioned node(s) by issuing:</p> <pre><code> clues status\n</code></pre> <p>which obtains:</p> <pre><code>| node            |state| enabled |time stable|(cpu,mem) used |(cpu,mem) total|\n|-----------------|-----|---------|-----------|---------------|---------------|\n| wn1.localdomain | used| enabled | 00h00'49\" | 0.0,825229312 | 1,1992404992  |\n| wn2.localdomain | off | enabled | 00h06'43\" | 0,0           | 1,1073741824  |\n| wn3.localdomain | off | enabled | 00h06'43\" | 0,0           | 1,1073741824  |\n| wn4.localdomain | off | enabled | 00h06'43\" | 0,0           | 1,1073741824  |\n| wn5.localdomain | off | enabled | 00h06'43\" | 0,0           | 1,1073741824  |\n</code></pre> <p>The working nodes transition from <code>off</code> to <code>powon</code> and, finally, to the <code>used</code> status.</p>"},{"location":"deploy-ec3/#default-service-endpoints","title":"Default Service Endpoints","text":"<p>Once the OSCAR framework is running on the Kubernetes cluster, the endpoints described in the following table should be available. Most of the passwords/tokens are dynamically generated at deployment time and made available in the <code>/var/tmp</code> folder of the front-end node of the cluster.</p> Service Endpoint Default User Password File OSCAR https://{FRONT_NODE} oscar oscar_password MinIO https://{FRONT_NODE}:30300 minio minio_secret_key OpenFaaS http://{FRONT_NODE}:31112 admin gw_password Kubernetes API https://{FRONT_NODE}:6443 tokenpass Kube. Dashboard https://{FRONT_NODE}:30443 dashboard_token <p>Note that <code>{FRONT_NODE}</code> refers to the public IP of the front-end of the Kubernetes cluster.</p> <p>For example, to get the OSCAR password, you can execute:</p> <pre><code>./ec3 ssh oscar-cluster cat /var/tmp/oscar_password\n</code></pre>"},{"location":"deploy-helm/","title":"Deployment with Helm","text":"<p>OSCAR can also be deployed on any existing Kubernetes cluster through its helm chart. However, to make the platform work properly, the following dependencies must be satisfied.</p> <ul> <li>A StorageClass with the <code>ReadWriteMany</code> access mode must be configured in     the cluster for the creation of the persistent volume mounted on the service     containers. For this purpose, we use the     Kubernetes NFS-Client Provisioner,     but there are other     volume plugins     that support this access mode.</li> <li>MinIO must be deployed and properly configured in the cluster. Its     helm chart can be used     for this purpose. It is important to configure it properly to have access from     inside and outside the cluster, as the OSCAR's web interface connects directly     to its API. In the OSCAR helm chart, you must indicate the     values     corresponding to its credentials and endpoint.</li> </ul>"},{"location":"deploy-im-dashboard/","title":"Deployment with IM","text":"<p>An OSCAR cluster can be easily deployed on multiple Cloud platforms via the Infrastructure Manager's Dashboard (IM Dashboard). This is a managed service provided by EGI and operated by the GRyCAP research group at the Universitat Polit\u00e8cnica de Val\u00e8ncia to deploy customized virtual infrastructures across many Cloud providers.</p> <p>Using the IM Dashboard is the easiest and most convenient approach to deploy an OSCAR cluster. It also automatically allocates a DNS entry and TLS certificates to support HTTPS-based access to the OSCAR cluster and companion services (e.g. MinIO).</p> <p>This example shows how to deploy an OSCAR cluster on Amazon Web Services (AWS) with two nodes. Thanks to the IM, the very same procedure allows to deploy the OSCAR cluster in an on-premises Cloud (such as OpenStack) or any other Cloud provider supported by the IM.</p> <p>These are the steps:</p> <ol> <li> <p>Access the IM Dashboard</p> <p></p> <p>You will need to authenticate via EGI Check-In, which supports mutiple Identity Providers (IdP). There is no need to register and the service is provided free of charge.</p> </li> <li> <p>Configure the Cloud Credentials</p> <p>Once logged in, you need to define the access credentials to the Cloud on which the OSCAR cluster will be deployed. These should be temporary credentials under the principle of least privilege (PoLP).</p> <p></p> <p></p> <p></p> <p>In our case, we indicate an identifier for the set of credentials, the Access Key ID and the Secret Access Key for an IAM user that has privileges to deploy Virtual Machines in Amazon EC2. With the default values indicated in this tutorial, you will need privileges to deploy the following instance types: <code>t3a.xlarge</code> for the front-end node and <code>t3a.medium</code> for the working node.</p> </li> <li> <p>Select the OSCAR template</p> <p></p> </li> </ol> <p>There are optional features than can be included in the OSCAR cluster to fit particular user needs. We'll skip them.</p> <p></p> <ol> <li> <p>Customize and deploy the OSCAR cluster</p> <p>In this panel you can specify the number of Working Nodes (WNs) of the cluster together with the computational requirements for each node. We leave the default values.</p> <ul> <li>Number of WNs in the OSCAR cluster.</li> <li>Number of CPUs for the front-end node.</li> <li>Amount of Memory (RAM) for the front-end node</li> <li>Flavor name of the front-end node. This is only required in case of special flavors (i.e. with GPUs): Instance type that will be selected for the front-end node.</li> <li>Number of CPUs for the WNs (Working Nodes).</li> <li>Amount of Memory (RAM) for the WNs.</li> <li>Flavor name of the WNs. Again, this is only required in case of special flavors</li> <li>Size of the extra HD (Hard Disk) added to the node. </li> </ul> <p>In the following panel, specify the passwords to be employed to access the Kubernetes Web UI (Dashboard), to access OSCAR and to access the MinIO dashboard. These passwords/tokens can also be used for programmatic access to the respective services.</p> <ul> <li>Access Token for the Kubernetes admin user: Used to connect to the Kubernetes Dashboard.</li> <li>OSCAR password: To log in to the OSCAR cluster as an admin user.</li> <li>MinIO password (8 characters min.).</li> <li>Email to be used in the Let's Encrypt issuer.</li> <li>ID of the user that creates the infrastructure.</li> <li>VO (Virtual Organization) to support: It supports OIDC (OpenID Connect) log in. If empty, only the user who deploys the cluster can log in. If a VO is specified, all the members of the VO can log in the OSCAR cluster.</li> <li>Flag to add NVIDIA support: if you want to use NVIDIA.</li> <li>Flag to install Apache YuniKorn: if you are going to use YuniKorn. </li> </ul> <p>Now, choose the Cloud provider. The ID specified when creating the Cloud credentials will be shown. You will also need to specify the Amazon Machine Image (AMI) identifier. We chose an AMI based on Ubuntu 20.04 provided by Canonical whose identifier for the us-east-1 region is: ami-09e67e426f25ce0d7</p> <p>NOTE: You should obtain the AMI identifier for the latest version of the OS. This way, security patches will be already installed. You can obtain this AMI identifier from the AWS Marketplace or the Amazon EC2 service.</p> <p></p> <p>Give the infrastructure a name and press \"Submit\".</p> </li> <li> <p>Check the status of the deployment OSCAR cluster</p> <p>You will see that the OSCAR cluster is being deployed and the infrastructure reaches the status \"running\". The process will finish when it reaches the state \"configured\".</p> <p></p> <p>If you are interested in understanding what is happening under the hood you can see the logs:</p> <p></p> </li> <li> <p>Accessing the OSCAR cluster</p> <p>Once reached the \"configured\" state, see the \"Outputs\" to obtain the different endpoints:</p> <ul> <li>console_minio_endpoint: To access the MinIO web UI.</li> <li>dashboard_endpoint: To access the Kubernetes dashboard.</li> <li>local_oscarui_endpoint: To access the OSCAR Dashboard. It supports username/password authentication.</li> <li>minio_endpoint: Endpoint where the MinIO API is listening. If you     access it through a web browser, you will be redirected to     \"console_minio_endpoint\".</li> <li>oscarui_endpoint: To access the OSCAR Dashboard. This one supports both username/password authentication and authentication via EGI Check-In for the user who deployed the OSCAR cluster and the users belonging to the VO specified at deployment time, if any.  </li> </ul> <p></p> <p>The OSCAR Dashboard can be accessed with the username <code>oscar</code> and the password you specified at deployment time.</p> <p></p> <p>The MinIO UI can be accessed with the username <code>minio</code> and the password you specified at deployment time.</p> <p></p> <p>The Kubernetes Dashboard can be accessed with the token you specified at deployment time. </p> <p>You can obtain statistics about the Kubernetes cluster: </p> </li> <li> <p>Terminating the OSCAR cluster</p> <p>You can terminate the OSCAR cluster from the IM Dashboard: </p> </li> </ol>"},{"location":"deploy-k3s-ansible/","title":"Deployment on K3s with Ansible","text":"<p>The folder <code>deploy/ansible</code> contains all the necessary files to deploy a K3s cluster together with the OSCAR platform using Ansible. This way, a minified Kubernetes distribution can be used to configure OSCAR on IoT devices located at the Edge, such as Raspberry PIs. Note that this playbook can also be applied to quickly spread the OSCAR platform on top of any machine or already started cloud instance since the playbook is compatible with GNU/Linux on ARM64 and AMD64 architectures.</p>"},{"location":"deploy-k3s-ansible/#requirements","title":"Requirements","text":"<p>In order to use the playbook, you must install the following components:</p> <ul> <li>Ansible, following this guide.</li> <li>The <code>netaddr</code>   python library.</li> <li>OpenSSH, to remotely access the hosts to be configured.</li> </ul>"},{"location":"deploy-k3s-ansible/#usage","title":"Usage","text":""},{"location":"deploy-k3s-ansible/#clone-the-folder","title":"Clone the folder","text":"<p>First of all, you must clone the OSCAR repo:</p> <pre><code>git clone https://github.com/grycap/oscar.git\n</code></pre> <p>And place into the <code>ansible</code> directory:</p> <pre><code>cd oscar/deploy/ansible\n</code></pre>"},{"location":"deploy-k3s-ansible/#ssh-configuration","title":"SSH configuration","text":"<p>As Ansible is an agentless automation tool, you must configure the <code>~/.ssh/config</code> file for granting access to the hosts to be configured via the SSH protocol. This playbook will use the <code>Host</code> field from SSH configuration to set the hostnames of the nodes, so please take care of naming them properly.</p> <p>Below you can find an example of a configuration file for four nodes, being the <code>front</code> the only one with a public IP, so it will be used as a proxy for the SSH connection to the working nodes (<code>ProxyJump</code> option) via its internal network.</p> <pre><code>Host front\n  HostName &lt;PUBLIC_IP&gt;\n  User ubuntu\n  IdentityFile ~/.ssh/my_private_key\n\nHost wn1\n  HostName &lt;PRIVATE_IP&gt;\n  User ubuntu\n  IdentityFile ~/.ssh/my_private_key\n  ProxyJump front\n\nHost wn2\n  HostName &lt;PRIVATE_IP&gt;\n  User ubuntu\n  IdentityFile ~/.ssh/my_private_key\n  ProxyJump front\n\nHost wn3\n  HostName &lt;PRIVATE_IP&gt;\n  User ubuntu\n  IdentityFile ~/.ssh/my_private_key\n  ProxyJump front\n</code></pre>"},{"location":"deploy-k3s-ansible/#configuration-of-the-inventory-file","title":"Configuration of the inventory file","text":"<p>Now, you have to edit the <code>hosts</code> file and add the hosts to be configured. Note that only one node must be set in the <code>[front]</code> section, while one or more nodes can be configured as working nodes of the cluster in the <code>[wn]</code> section. For example, for the previous SSH configuration the <code>hosts</code> inventory file should look like this:</p> <pre><code>[front]\n; Put here the frontend node as defined in .ssh/config (Host)\nfront\n\n[wn]\n; Put here the working nodes (one per line) as defined in the .ssh/config (Host)\nwn1\nwn2\nwn3\n</code></pre>"},{"location":"deploy-k3s-ansible/#setting-up-the-playbook-variables","title":"Setting up the playbook variables","text":"<p>You also need to set up some parameters for the configuration of the cluster and OSCAR components, like OSCAR and MinIO credentials and DNS endpoints to configure the Kubernetes Ingress and cert-manager to securely expose the services. To do it, please edit the <code>vars.yaml</code> file and update the variables:</p> <pre><code>---\n# K3s version to be installed\nkube_version: v1.22.3+k3s1\n# Token to login in K3s and the Kubernetes Dashboard\nkube_admin_token: kube-token123\n# Password for OSCAR\noscar_password: oscar123\n# DNS name for the OSCAR Ingress and Kubernetes Dashboard (path \"/dashboard/\")\ndns_host: oscar-cluster.example.com\n# Password for MinIO\nminio_password: minio123\n# DNS name for the MinIO API Ingress\nminio_dns_host: minio.oscar-cluster.example.com\n# DNS name for the MinIO Console Ingress\nminio_dns_host_console: minio-console.oscar-cluster.example.com\n</code></pre>"},{"location":"deploy-k3s-ansible/#installation-of-the-required-ansible-roles","title":"Installation of the required ansible roles","text":"<p>To install the required roles you only have to run:</p> <pre><code>ansible-galaxy install -r install_roles.yaml --force\n</code></pre> <p>The <code>--force</code> argument ensures you have the latest version of the roles.</p>"},{"location":"deploy-k3s-ansible/#running-the-playbook","title":"Running the playbook","text":"<p>Finally, with the following command the ansible playbook will be executed, configuring the nodes set in the <code>hosts</code> inventory file:</p> <pre><code>ansible-playbook -i hosts oscar-k3s.yaml\n</code></pre>"},{"location":"devel-docs/","title":"Documentation development","text":"<p>OSCAR uses MKDocs for the documentation. In particular, Material for MKDocs.</p> <p>Install the following dependencies:</p> <pre><code>pip install mkdocs mkdocs-material mkdocs-render-swagger-plugin\n</code></pre> <p>The from the main folder <code>oscar</code> run:</p> <pre><code>mkdocs serve\n</code></pre> <p>The documentation will be available in http://127.0.0.1:8000</p>"},{"location":"exposed-services/","title":"Exposed Services","text":"<p>OSCAR supports the deployment and elasticity management of long-running stateless services whose internal API or web-based UI must be directly reachable outside the cluster. </p> <p>\u2139\ufe0f</p> <p>This functionality can be used to support the fast inference of pre-trained AI models that require close to real-time processing with high throughput.  In a traditional serverless approach, the AI model weights would be loaded in memory for each service invocation. Exposed services are also helpful when stateless services created out of large containers require too much time to start processing a service invocation. By exposing an OSCAR service, the AI model weights could be loaded just once, and the service would perform the AI model inference for each subsequent request.</p> <p></p> <p>An auto-scaled load-balanced approach for these stateless services is supported. When the average CPU exceeds a certain user-defined threshold, additional service pods are dynamically created (and removed when no longer necessary) within the user-defined boundaries. The user can also define the minimum and maximum replicas of the service to be present on the cluster (see the parameters <code>min_scale</code> and <code>max_scale</code> in ExposeSettings).</p>"},{"location":"exposed-services/#prerequisites-in-the-container-image","title":"Prerequisites in the container image","text":"<p>The container image needs to have an HTTP server that binds to a specific port (see the parameter <code>port</code> in ExposeSettings). If developing a service from scratch in Python, you can use FastAPI or Flask to create an API. In Go, you can use Gin. For Ruby, you can use Sinatra. </p> <p>\u26a0\ufe0f</p> <p>If the service exposes a web-based UI, you must ensure that the content cannot only be served from the root document ('/') since the service will be exposed in a certain subpath. </p>"},{"location":"exposed-services/#how-to-define-an-exposed-oscar-service","title":"How to define an exposed OSCAR service","text":"<p>The minimum definition to expose an OSCAR service is to indicate in the corresponding FDL the port inside the container where the service will be listening.</p> <pre><code>expose:\n  api_port: 5000\n</code></pre> <p>Once the service is deployed, you can check if it was created correctly by making an HTTP request to the exposed endpoint: </p> <pre><code>https://{oscar_endpoint}/system/services/{service_name}/exposed/{path_resource} \n</code></pre> <p>Notice that if you get a <code>502 Bad Gateway</code> error, it is most likely because the specified port on the service does not match the API port.</p> <p>Additional options can be defined in the \"expose\" section of the FDL (some previously mentioned), such as:</p> <ul> <li><code>min_scale</code>: The minimum number of active pods (default: 1).</li> <li><code>max_scale</code>: The maximum number of active pods (default: 10) or the CPU threshold, which, once exceeded, will trigger the creation of additional pods (default: 80%).</li> <li><code>rewrite_target</code>: Target the URI where the traffic is redirected (default: false).</li> <li><code>NodePort</code>: The access method from the domain name to the public ip <code>&lt;cluster_ip&gt;:&lt;NodePort&gt;</code>.</li> <li><code>default_command</code>: Selects between executing the container's default command and executing the script inside the container. (default: false, it executes the script)</li> <li><code>set_auth</code>: The credentials are composed of the service name as the user and the service token as the password. Turn off this field if the container provides its own authentication method. It does not work with <code>NodePort</code> (default: false, it has no authentication).</li> </ul> <p>Below is an example of the <code>expose</code> section of the FDL, showing that there will be between 5 to 15 active pods and that the service will expose an API in port 4578. The number of active pods will grow when the use of CPU increases by more than 50% and the active pods will decrease when the CPU use decreases below that threshold.</p> <pre><code>expose:\n  min_scale: 5 \n  max_scale: 15 \n  api_port: 4578  \n  cpu_threshold: 50\n  set_auth: true\n  rewrite_target: true\n  default_command: true\n</code></pre> <p>In addition, you can see below a full example of a recipe to expose a service from the AI4EOSC Marketplace:</p> <pre><code>functions:\n  oscar:\n  - oscar-cluster:\n     name: body-pose-detection\n     memory: 2Gi\n     cpu: '1.0'\n     image: deephdc/deep-oc-posenet-tf\n     script: script.sh\n     environment:\n        Variables:\n          INPUT_TYPE: json  \n     expose:\n      min_scale: 1 \n      max_scale: 10 \n      api_port: 5000  \n      cpu_threshold: 20 \n      set_auth: true\n     input:\n     - storage_provider: minio.default\n       path: body-pose-detection/input\n     output:\n     - storage_provider: minio.default\n       path: body-pose-detection/output\n</code></pre> <p>So, to invoke the API of this example the request will need the following information,</p> <ol> <li>OSCAR endpoint. <code>localhost</code> or <code>https://{OSCAR_endpoint}</code></li> <li>Path resource. In this case, it is <code>v2/models/posenetclas/predict/</code>. Please do not forget the final <code>/</code></li> <li>Use <code>-k</code> or <code>--insecure</code> if the SSL is false.</li> <li>Input image with the name <code>people.jpeg</code></li> <li>Output. It will create a <code>.zip</code> file that has the outputs</li> </ol> <p>and will end up looking like this:</p> <pre><code>curl {-k} -X POST https://{oscar_endpoint}/system/services/body-pose-detection-async/exposed/{path resource} -H  \"accept: */*\" -H  \"Content-Type: multipart/form-data\" -F \"data=@{input image};type=image/png\" --output {output file}\n</code></pre> <p>Finally, the complete command that works in Local Testing with an image called <code>people.jpeg</code> as input and <code>output_posenet.zip</code> as output.</p> <pre><code>curl -X POST https://localhost/system/services/body-pose-detection-async/exposed/v3/models/posenetclas/predict/ -H  \"accept: */*\" -H  \"Content-Type: multipart/form-data\" -F \"data=@people.jpeg;type=image/png\" --output output_posenet.zip\n</code></pre> <p>Another FDL example shows how to expose a simple NGINX server as an OSCAR service:</p> <pre><code>functions:\n  oscar:\n  - oscar-cluster:\n     name: nginx\n     memory: 2Gi\n     cpu: '1.0'\n     image: nginx\n     script: script.sh\n     expose:\n      min_scale: 2 \n      max_scale: 10 \n      api_port: 80  \n      cpu_threshold: 50 \n</code></pre> <p>In case you use the NGINX example above in your local OSCAR cluster, you will see the nginx welcome page in: <code>http://localhost/system/services/nginx/exposed/</code>. Two active pods of the deployment will be shown with the command <code>kubectl get pods -n oscar-svc</code></p> <pre><code>oscar-svc            nginx-dlp-6b9ddddbd7-cm6c9                         1/1     Running     0             2m1s\noscar-svc            nginx-dlp-6b9ddddbd7-f4ml6                         1/1     Running     0             2m1s\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Sometimes, when trying to deploy the cluster locally, it tells me that the :80 port is already in use.</li> </ul> <p>You may have a server running on the :80 port, such as Apache, while the deployment is trying to use it for the OSCAR Dashboard. Restarting it would solve this problem.</p> <ul> <li>I get the following error message: \"Unable to communicate with the cluster. Please make sure that the endpoint is well-typed and accessible.\"</li> </ul> <p>When using oscar-cli, you can get this error if you try to run a service that is not present on the cluster set as default. You can check if you are using the correct default cluster with the following command,</p> <p><code>oscar-cli cluster default</code></p> <p>and set a new default cluster with the following command:</p> <p><code>oscar-cli cluster default -s CLUSTER_ID</code></p> <ul> <li>How do I use a secret image?</li> </ul> <p>In case it is required the use of secret images, you should create a secret with the docker login configuration with a structure like this:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: dockersecret\n  namespace: oscar-svc\ndata:\n  .dockerconfigjson: {base64 .docker/config.json}\ntype: kubernetes.io/dockerconfigjson\n</code></pre> <p>Apply the file through kubectl into the Kubernetes OSCAR cluster to create the secret. To use it in OSCAR services, you must add the secret name (<code>dockersecret</code> in this example) in the definition of the service, using the API or a FDL, under the <code>image_pull_secrets</code> parameter, or through the \"Docker secret\" field in OSCAR-Dasboard.</p> <ul> <li>The OSCAR cluster does not have TLS certificates (HTTPS). Why?</li> </ul> <p>It could happen when an OSCAR cluster is deployed from an IM recipe that does not have certificates or the Let's Encrypt limit has been reached. Only 50 certificates per week can be issued. Those certificates have a 90 days expiration lifetime. The certificates issued can be seen at https://crt.sh/?q=im.grycap.net.</p> <ul> <li>I do not have certificates. I can not see the buckets. What do I have to do?</li> </ul> <p>If the OSCAR cluster has no certificate OSCAR Dashboard will not show the buckets.</p> <p></p> <p>You can fix this by entering in the MinIO endpoint <code>minio.&lt;OSCAR-endpoint&gt;</code>. The browser will block the page because it is unsafe. Once you accept the risk, you will enter the MinIO page. It is not necessary to log in.</p> <p></p> <p>Return to OSCAR Dashboard and, then,  you can see the buckets. The buckets will be shown only in the browser you do this process. The results may vary depending on the browser. For example, they will show up in Firefox but not in Chrome.</p> <p></p>"},{"location":"fdl-composer/","title":"FDL Composer","text":"<p>OSCAR Services can be aggregated into data-driven workflows where the output data of one service is stored in the object store that triggers another service, potentially in a different OSCAR cluster. This allows to execute the different phases of the workflow in disparate computing infrastructures.</p> <p>However, writing an entire workflow in an FDL file can be a difficult task for some users.</p> <p>To simplify the process you can use FDL Composer, a web-based application to facilitate the definition of FDL YAML files for OSCAR and SCAR.</p> <p></p>"},{"location":"fdl-composer/#how-to-access-fdl-composer","title":"How to access FDL Composer","text":"<p>Just access FDL Composer which is a Single Page Application (SPA) running entirely in your browser. If you prefer to execute it on your computer instead of using the web, clone the git repository by using the following command:</p> <pre><code>git clone https://github.com/grycap/fdl-composer\n</code></pre> <p>And the run the app with <code>npm</code>:</p> <pre><code>npm start\n</code></pre>"},{"location":"fdl-composer/#basic-elements","title":"Basic elements","text":"<p>Workflows are composed of <code>OSCAR services</code> and <code>Storage providers</code>:</p>"},{"location":"fdl-composer/#oscar-services","title":"OSCAR services","text":"<p><code>OSCAR services</code> are responsible for processing the data uploaded to <code>Storage providers</code>.</p> <p>Defining a new <code>OSCAR service</code>  requires filling at least the <code>name</code>, <code>image</code>, and <code>script</code> fields.</p> <p>To define environment variables you must add them as a comma separated string of key=value entries. For example,  to create a variable with the name <code>firstName</code> and the value <code>John</code>, the \"Environment variables\" field should look like <code>firstName=John</code>. If you want to assign more than one variable, for example, <code>firstName</code> and <code>lastName</code> with the values <code>John</code> and <code>Keats</code>, the input field should include them all separated by commas (e.g., <code>firstName=John,lastName=Keats</code>).</p>"},{"location":"fdl-composer/#storage-providers-and-bucketsfolders","title":"Storage providers and buckets/folders","text":"<p><code>Storage providers</code> are object storage systems  responsible for storing both the input files to be processed by <code>OSCAR services</code> and the output files generated as a result of the processing.</p> <p>Three types of storage providers can be used in OSCAR FDLs: MinIO, Amazon S3, and OneData.</p> <p>To configure them, drag the storage provider from the menu to the canvas and double click on the item created. A window with a single input will appear. Then, insert the path of the folder name. To edit one of the storage providers, move the mouse over the item and select the edit option.</p> <p>Remember that only MinIO can be used as input storage provider for OSCAR services.</p>"},{"location":"fdl-composer/#download-and-load-state","title":"Download and load state","text":"<p>The defined workflow can be saved in a file using the \"Download state\" button. OSCAR services, Storage Providers, and Buckets are kept in the file. The graphic workflow can be edited later by loading it with the \"Load state\" button.</p>"},{"location":"fdl-composer/#create-a-yaml-file","title":"Create a YAML file","text":"<p>You can easily download the workflow's FDL file (in YAML) through the \"Export  YAML\" button.</p>"},{"location":"fdl-composer/#connecting-components","title":"Connecting components","text":"<p>All components have four ports: The up and left ones are input ports while the right and down ports are used as output. <code>OSCAR Services</code> can only be connected with <code>Storage providers</code>, always linked in the same direction (the output of one element with the input of the other).</p> <p>When two services are connected, both will be declared in the FDL file, but they will work separately, and there will be no workflow between them. If two storage providers are connected between them, it will have no effect, but both storages will be declared.</p>"},{"location":"fdl-composer/#scar-options","title":"SCAR options","text":"<p>FDL Composer can also create FDL files for SCAR. This allows to define workflows that can be executed on the Edge or in on-premises Clouds through OSCAR, and on the public Cloud (AWS Lambda and/or AWS Batch) through SCAR.</p>"},{"location":"fdl-composer/#example","title":"Example","text":"<p>There is an example of FDL Composer implementing the video-process use case in our blog.</p>"},{"location":"fdl/","title":"Functions Definition Language (FDL)","text":"<p>OSCAR services are typically defined via the Functions Definition Language (FDL) to be deployed via the OSCAR CLI. Alternative approaches are using the web-based wizard in the OSCAR Dashboard or, for a programmatic integration, via the OSCAR API. </p> <p>\u2139\ufe0f</p> <p>It is called Functions Definition Language instead of Services Definition Language, because the definition was initially designed for SCAR, which supports Lambda functions.</p> <p>Example:</p> <pre><code>functions:\n  oscar:\n  - oscar-test:\n      name: plants\n      memory: 2Gi\n      cpu: '1.0'\n      image: grycap/oscar-theano-plants\n      script: plants.sh\n      isolation_level: user\n      visibility: restricted\n      allowed_users: \n      - \"62bb11b40398f7.......926@egi.eu\"\n      - \"5e14d33ac4abc9.......463@egi.eu\"\n      input:\n      - storage_provider: minio.default\n        path: example-workflow/in\n      output:\n      - storage_provider: minio.default\n        path: example-workflow/med\n  - oscar-test:\n      name: grayify\n      memory: 1Gi\n      cpu: '1.0'\n      image: grycap/imagemagick\n      script: grayify.sh\n      interlink_node_name: vega-new-vk\n      expose:\n        min_scale: 3 \n        max_scale: 7 \n        port: 5000  \n        cpu_threshold: 70 \n        nodePort: 30500\n        set_auth: true\n        rewrite_target: true\n        default_command: true\n      input:\n      - storage_provider: minio.default\n        path: example-workflow/med\n      output:\n      - storage_provider: minio.default\n        path: example-workflow/res\n      - storage_provider: onedata.my_onedata\n        path: result-example-workflow\n      - storage_provider: webdav.dcache\n        path: example-workflow/res\n\nstorage_providers:\n  onedata:\n    my_onedata:\n      oneprovider_host: my_provider.com\n      token: my_very_secret_token\n      space: my_onedata_space\n  webdav:\n    dcache:\n      hostname: my_dcache.com\n      login: my_username\n      password: my_password\n  rucio:\n    rucio:\n      host: rucio_host\n      auth_host: rucio_auth_host\n      account: rucio_account\n      rse: rucio_rse\n      refresh_token: oidc_refresh_token\n      oidc_audience: oidc_audience\n      token_endpoint: oidc_token_endpoint\n\n</code></pre>"},{"location":"fdl/#top-level-parameters","title":"Top level parameters","text":"Field Description <code>functions</code> Functions Mandatory parameter to define a Functions Definition Language file. Note that \"functions\" instead of \"services\" has been used in order to keep compatibility with SCAR <code>storage_providers</code> StorageProviders Parameter to define the credentials for the storage providers to be used in the services <code>clusters</code> map[string]Cluster Configuration for the OSCAR clusters that can be used as service's replicas, being the key the user-defined identifier for the cluster. Optional"},{"location":"fdl/#functions","title":"Functions","text":"Field Description <code>oscar</code> map[string]Service array Main object with the definition of the OSCAR services to be deployed. The components of the array are Service maps, where the key of every service is the identifier of the cluster where the service (defined as the value of the entry on the map) will be deployed."},{"location":"fdl/#service","title":"Service","text":"Field Description <code>name</code> string The name of the service <code>cluster_id</code> string Identifier for the current cluster, used to specify the cluster's StorageProvider in job delegations. OSCAR-CLI sets it using the cluster_id from the FDL. Optional. (default: \"\") <code>image</code> string Docker image for the service <code>vo</code> string Virtual Organization (VO) in which the user creating the service is enrolled. (Required for multitenancy) <code>allowed_users</code> string array Array of EGI UIDs to grant specific user permissions on the service. If empty, the service is considered as accesible to all the users with access to the OSCAR cluster. (Enabled since OSCAR version v3.0.0). <code>alpine</code> boolean Set if the Docker image is based on Alpine. If <code>true</code>, a custom release of the faas-supervisor will be used. Optional (default: false) <code>script</code> string Local path to the user script to be executed inside the container created out of the service invocation <code>file_stage_in</code> bool Skip the download of the input files by the faas-supervisor (default: false) <code>image_pull_secrets</code> string array Array of Kubernetes secrets. Only needed to use private images located on private registries. <code>memory</code> string Memory limit for the service following the kubernetes format. Optional (default: 256Mi) <code>cpu</code> string CPU limit for the service following the kubernetes format. Optional (default: 0.2) <code>enable_gpu</code> bool Enable the use of GPU. Requires a device plugin deployed on the cluster (More info: Kubernetes device plugins). Optional (default: false) <code>enable_sgx</code> bool Enable the use of SGX plugin on the cluster containers. (More info: SGX plugin documentation). Optional (default: false) <code>image_prefetch</code> bool Enable the use of image prefetching (retrieve the container image in the nodes when creating the service). Optional (default: false) <code>total_memory</code> string Limit for the memory used by all the service's jobs running simultaneously. Apache YuniKorn's' scheduler is required to work. Same format as Memory, but internally translated to MB (integer). Optional (default: \"\") <code>total_cpu</code> string Limit for the virtual CPUs used by all the service's jobs running simultaneously. Apache YuniKorn's' scheduler is required to work. Same format as CPU, but internally translated to millicores (integer). Optional (default: \"\") <code>synchronous</code> SynchronousSettings Struct to configure specific sync parameters. This settings are only applied on Knative ServerlessBackend. Optional. <code>expose</code> ExposeSettings Allows to expose the API or UI of the application run in the OSCAR service outside of the Kubernetes cluster. Optional. <code>replicas</code> Replica array List of replicas to delegate jobs. Optional. <code>rescheduler_threshold</code> string Time (in seconds) that a job (with replicas) can be queued before delegating it. Optional. <code>log_level</code> string Log level for the faas-supervisor. Available levels: NOTSET, DEBUG, INFO, WARNING, ERROR and CRITICAL. Optional (default: INFO) <code>input</code> StorageIOConfig array Array with the input configuration for the service. Optional <code>output</code> StorageIOConfig array Array with the output configuration for the service. Optional <code>environment</code> EnvVarsMap The user-defined environment variables assigned to the service. Optional <code>annotations</code> map[string]string User-defined Kubernetes annotations to be set in job's definition. Optional <code>labels</code> map[string]string User-defined Kubernetes labels to be set in job's definition. Optional <code>interlink_node_name</code> string Name of the virtual kubelet node (if you are using InterLink nodes) Optional <code>isolation_level</code> string Select the isolation level of the MinIO buckets: <code>SERVICE</code> or <code>USER</code> (<code>SERVICE</code> by default) Optional <code>visibility</code> string Select the visibility level of service: <code>private</code>, <code>restricted</code> or <code>public</code> (<code>private</code> by default) Optional"},{"location":"fdl/#synchronoussettings","title":"SynchronousSettings","text":"Field Description <code>min_scale</code> integer Minimum number of active replicas (pods) for the service. Optional. (default: 0) <code>max_scale</code> integer Maximum number of active replicas (pods) for the service. Optional. (default: 0 (Unlimited))"},{"location":"fdl/#exposesettings","title":"ExposeSettings","text":"Field Description <code>min_scale</code> integer Minimum number of active replicas (pods) for the service. Optional. (default: 1) <code>max_scale</code> integer Maximum number of active replicas (pods) for the service. Optional. (default: 10 (Unlimited)) <code>port</code> integer Port inside the container where the API is exposed. (value: 0 , the service wont be exposed.) <code>cpu_threshold</code> integer Percent of use of CPU before creating other pod (default: 80 max:100). Optional. <code>nodePort</code> integer Change the access method from the domain name to the public ip. Optional. <code>set_auth</code> bool Create credentials for the service, composed of the service name as the user and the service token as the password. (default: false). Optional. <code>rewrite_target</code> bool Target the URI where the traffic is redirected. (default: false). Optional. <code>default_command</code> bool Select between executing the container's default command and executing the script inside the container. (default: false). Optional."},{"location":"fdl/#mountsettings","title":"MountSettings","text":"Field Description <code>storage_provider</code> string Identifier of the storage provider. Optional. <code>path</code> string Path to the folder that will be mounted. Optional."},{"location":"fdl/#replica","title":"Replica","text":"Field Description <code>type</code> string Type of the replica to re-send events (can be <code>oscar</code> or <code>endpoint</code>) <code>cluster_id</code> string Identifier of the cluster as defined in the \"clusters\" FDL field. Only used if Type is <code>oscar</code> <code>service_name</code> string Name of the service in the replica cluster. Only used if Type is <code>oscar</code> <code>url</code> string URL of the endpoint to re-send events (HTTP POST). Only used if Type is <code>endpoint</code> <code>ssl_verify</code> boolean Parameter to enable or disable the verification of SSL certificates. Only used if Type is <code>endpoint</code>. Optional. (default: true) <code>priority</code> integer Priority value to define delegation priority. Highest priority is defined as 0. If a delegation fails, OSCAR will try to delegate to another replica with lower priority. Optional. (default: 0) <code>headers</code> map[string]string Headers to send in delegation requests. Optional"},{"location":"fdl/#storageioconfig","title":"StorageIOConfig","text":"Field Description <code>storage_provider</code> string Reference to the storage provider defined in storage_providers. This string is composed by the provider's name (minio, s3, onedata) and the identifier (defined by the user), separated by a point (e.g. \"minio.myidentifier\") <code>path</code> string Path in the storage provider. In MinIO and S3 the first directory of the specified path is translated into the bucket's name (e.g. \"bucket/folder/subfolder\") <code>suffix</code> string array Array of suffixes for filtering the files to be uploaded. Only used in the <code>output</code> field. Optional <code>prefix</code> string array Array of prefixes for filtering the files to be uploaded. Only used in the <code>output</code> field. Optional"},{"location":"fdl/#envvarsmap","title":"EnvVarsMap","text":"Field Description <code>variables</code> map[string]string Map to define the environment variables that will be available in the service container <code>secrets</code> map[string]string Map to define the secret environment variables that will be available in the service container"},{"location":"fdl/#storageproviders","title":"StorageProviders","text":"Field Description <code>minio</code> map[string]MinIOProvider Map to define the credentials for a MinIO storage provider, being the key the user-defined identifier for the provider <code>s3</code> map[string]S3Provider Map to define the credentials for an Amazon S3 storage provider, being the key the user-defined identifier for the provider <code>onedata</code> map[string]OnedataProvider Map to define the credentials for a Onedata storage provider, being the key the user-defined identifier for the provider <code>webdav</code> map[string]WebDavProvider Map to define the credentials for a storage provider accesible via WebDAV protocol, being the key the user-defined identifier for the provider <code>rucio</code> map[string]RucioProvider Map to define the credentials for a Rucio storage provider, being the key the user-defined identifier for the provider"},{"location":"fdl/#cluster","title":"Cluster","text":"Field Description <code>endpoint</code>string Endpoint of the OSCAR cluster API <code>auth_user</code>string Username to connect to the cluster (basic auth) <code>auth_password</code>string Password to connect to the cluster (basic auth) <code>ssl_verify</code>boolean Parameter to enable or disable the verification of SSL certificates"},{"location":"fdl/#minioprovider","title":"MinIOProvider","text":"Field Description <code>endpoint</code> string MinIO endpoint <code>verify</code> bool Verify MinIO's TLS certificates for HTTPS connections <code>access_key</code> string Access key of the MinIO server <code>secret_key</code> string Secret key of the MinIO server <code>region</code> string Region of the MinIO server"},{"location":"fdl/#s3provider","title":"S3Provider","text":"Field Description <code>access_key</code> string Access key of the AWS S3 service <code>secret_key</code> string Secret key of the AWS S3 service <code>region</code> string Region of the AWS S3 service"},{"location":"fdl/#onedataprovider","title":"OnedataProvider","text":"Field Description <code>oneprovider_host</code> string Endpoint of the Oneprovider <code>token</code> string Onedata access token <code>space</code> string Name of the Onedata space"},{"location":"fdl/#webdavprovider","title":"WebDAVProvider","text":"Field Description <code>hostname</code> string Provider hostname. It only works with <code>https</code> protocol. Skip the protocol. <code>login</code> string Provider account username <code>password</code> string Provider account password"},{"location":"fdl/#rucioprovider","title":"RucioProvider","text":"Field Description <code>host</code> string Rucio Provider hostname. It only works with <code>https</code> protocol. Skip the protocol. <code>auth_host</code> string Rucio authentication provider host <code>account</code> string Rucio account <code>rse</code> string Rucio rse <code>refresh_token</code> string OIDC refresh token <code>oidc_audience</code> string OIDC audience <code>token_endpoint</code> string OIDC issue endpoint"},{"location":"integration-compss/","title":"Integration with COMPSs","text":"<p>COMPSs is a task-based programming model which aims to ease the development of applications for distributed infrastructures, such as large High-Performance clusters (HPC), clouds and container managed clusters. COMPSs provides a programming interface for the development of the applications and a runtime system that exploits the inherent parallelism of applications at execution time.</p> <p>COMPSs support was introduced in OSCAR for the AI-SPRINT project to tackle the Personalized Healthcare use case in which OSCAR is employed to perform the inference phase of pre-trained models out of sensitive data captured from wearable devices. COMPSs, in particular, its Python binding named PyCOMPSs, was integrated to exploit parallelism across the multiple virtual CPUs of each pod resulting from each OSCAR service asynchronous invocation. This use case was coordinated by the Barcelona Supercomputing Center (BSC)</p> <p></p> <p>There are several examples that showcase the COMPSs integration with OSCAR in the examples/compss folder in GitHub.</p>"},{"location":"integration-egi/","title":"Integration with EGI","text":"<p>EGI is a federation of many cloud providers and hundreds of data centres, spread across Europe and worldwide that delivers advanced computing services to support scientists, multinational projects and research infrastructures.</p>"},{"location":"integration-egi/#deployment-on-the-egi-federated-cloud","title":"Deployment on the EGI Federated Cloud","text":"<p>The EGI Federated Cloud is an IaaS-type cloud, made of academic private clouds and virtualised resources and built around open standards. Its development is driven by requirements of the scientific communities.</p> <p>The OSCAR platform can be deployed on the EGI Federated Cloud resources through the IM Dashboard.</p> <p>You can follow EGI's IM Dashboard documentation or the OSCAR's IM Dasboard documentation.</p> <p></p>"},{"location":"integration-egi/#integration-with-egi-datahub-onedata","title":"Integration with EGI Datahub (Onedata)","text":"<p>EGI DataHub, based on Onedata, provides a global data access solution for science. Integrated with the EGI AAI, it allows users to have Onedata spaces supported by providers across Europe for replicated storage and on-demand caching.</p> <p>EGI DataHub can be used as an output storage provider for OSCAR, allowing users to store the resulting files of their OSCAR services on a Onedata space. This can be done thanks to the FaaS Supervisor. Used in OSCAR and SCAR, responsible for managing the data Input/Output and the user code execution.</p> <p>To deploy a function with Onedata as output storage provider you only have to specify an identifier, the URL of the Oneprovider host, your access token and the name of your Onedata space in the \"Storage\" tab of the service creation wizard:</p> <p></p> <p>And the path where you want to store the files in the \"OUTPUTS\" tab:</p> <p></p> <p>This means that scientists can store their output files on their Onedata space in the EGI DataHub for long-time persistence and easy sharing of experimental results between researchers.</p>"},{"location":"integration-egi/#integration-with-egi-check-in-oidc","title":"Integration with EGI Check-In (OIDC)","text":"<p>OSCAR API supports OIDC (OpenID Connect) access tokens to authorize users since release <code>v2.5.0</code>. By default, OSCAR clusters deployed via the IM Dashboard are configured to allow authorization via basic auth and OIDC tokens using the EGI Check-in issuer. From the IM Dashboard deployment window, users can add one EGI Virtual Organization to grant access for all users from that VO.</p> <p></p>"},{"location":"integration-egi/#accessing-from-oscar-dashboard","title":"Accessing from OSCAR Dashboard","text":"<p>The static web interface of OSCAR has been integrated with EGI Check-in and published in dashboard.oscar.grycap.net to facilitate the authorization of users. To login through EGI Check-In using OIDC tokens, users only have to put the endpoint of its OSCAR cluster and click on the \"EGI CHECK-IN\" button.</p> <p></p>"},{"location":"integration-egi/#integration-with-oscar-cli-via-oidc-agent","title":"Integration with OSCAR-CLI via OIDC Agent","text":"<p>Since version <code>v1.4.0</code> OSCAR CLI supports API authorization via OIDC tokens thanks to the integration with oidc-agent.</p> <p>Users must install <code>oidc-agent</code> following its instructions and create a new account configuration for the <code>https://aai.egi.eu/auth/realms/egi/</code> issuer. </p> <p>After that, clusters can be added with the command <code>oscar-cli cluster add</code> specifying the oidc-agent account name with the <code>--oidc-account-name</code> flag.</p>"},{"location":"integration-egi/#obtaining-an-access-token","title":"Obtaining an Access Token","text":"<p>Once logged in via EGI Check-In you can obtain an Access Token with one of this approaches:</p> <ul> <li> <p>From the command-line, using <code>oidc-agent</code> with the following command:</p> <p><code>sh oidc-token &lt;account-short-name&gt;</code> where <code>account-short-name</code> is the name of your account configuration.</p> </li> <li> <p>From the EGI Check-In Token Portal: https://aai.egi.eu/token</p> </li> </ul> <p></p>"},{"location":"integration-interlink/","title":"Integration with interLink","text":"<p>The project interLink is an open-source development that aims to provide an abstraction for executing a Kubernetes pod on any remote resource capable of managing a Container execution lifecycle.</p> <p>OSCAR uses the Kubernetes Virtual Node to translate a job request from the Kubernetes pod into a remote call. We have been using interLink to interact with an HPC cluster. For more infomation check this video demo.</p> <p></p>"},{"location":"integration-interlink/#installation-and-use-of-interlink-node-in-oscar-cluster","title":"Installation and use of interLink Node in OSCAR cluster","text":"<p>The cluster Kubernetes must have at least one virtual kubelet node. Those nodes will have tagged as <code>type=virtual-kubelet</code>. Follow the documentation in the interLink homepage to deploy an interLink virtual node to the Kubernetes cluster. OSCAR detects these nodes automatically.</p> <p>Once the Virtual node and OSCAR are installed correctly, you can use this node to offload your job to the configured remote host. To offload the jobs created by a service to an interLink node the name of the virtual node has to be set in the <code>interlink_node_name</code> variable of the service FDL.</p> <p>Otherwise, if the variable is not set, i.e., <code>\"\"</code>, the job will be executed in a normal Kubernetes node.</p>"},{"location":"integration-interlink/#annotations-restrictions-and-other-things-to-keep-in-mind","title":"Annotations, restrictions, and other things to keep in mind","text":"<ul> <li> <p>The OSCAR services annotations would be applied to every job for that service. The annotations are used to apply some configuration to the remote host.</p> </li> <li> <p>The memory and CPU defined in the OSCAR services are not applied to the jobs that are offloaded via interLink. Those parameters are set employing the annotations, as specified by the provider.</p> </li> <li> <p>When the offloading is set to a SLURM job, it's possible to request some resources employing slurm flags. For example: </p> <ul> <li>To request CPU and memory, <code>slurm-job.vk.io/flags</code>( <code>--job-name</code>, <code>--time=02:30:00</code>, <code>--cpus-per-task</code>, <code>--nodes</code>, <code>--mem</code>) </li> <li>To mount a system folder in an HPC cluster <code>job.vk.io/singularity-mounts</code> and value pattern <code>\"--bind &lt;outside-container&gt;:&lt;inside-container&gt;\"</code>. </li> <li>To  execute a command before each run <code>job.vk.io/pre-exec</code>.</li> </ul> </li> <li> <p>Any environment variable with a special character could create an error in the translation between the virtual node and the remote job. As a good practice, pass the environment variable encode in Base64 and decode it inside the execution of the script.</p> </li> <li> <p>Please note when the remote host is a SLURM cluster a docker container will be translated into a singularity one. There are some points to consider:</p> <ul> <li>You must reference the image container indicating <code>docker://</code> at the beginning, e.g., <code>docker://ghcr.io/intertwin-eu/itwinai:0.0.1-3dgan-0.2</code>. </li> <li>Once the image is pulled in the remote host, the image can be referenced by path <code>&lt;path-of-container&gt;/&lt;image&gt;.sif</code>.</li> <li>Note that your script will not run as a privileged user in the container. Therefore, you cannot write in the regular file system. Use the <code>/tmp</code> folder if needed instead.</li> <li>There is no working directory in singularity. Therefore, absolute paths are recommended.</li> </ul> </li> </ul> <p>The interLink integration was developed in the context of the interTwin project, with support from Istituto Nazionale di Fisica Nucleare - INFN, who developed interLink, and CERN, who provided the development of itwinai, used as a platform for advanced AI/ML workflows in digital twin applications and a use case. Special thanks to the IZUM Center in Slovenia for providing access to the HPC Vega supercomputing facility to perform the testing.</p>"},{"location":"integration-node-red/","title":"Integration with Node-RED","text":"<p>Node-RED is a flow-based development tool for visual programming, widely used to connect hardware devices, APIs, and online services. Its browser-based editor makes creating and deploying automation workflows simple.</p> <p>In the context of OSCAR integration, Node-RED can be deployed and configured to interact with OSCAR services using authentication tokens obtained after logging in or using tokens specific for the service.</p>"},{"location":"integration-node-red/#simple-tutorial","title":"Simple tutorial","text":"<p>This tutorial demonstrates a simple workflow that calls the YOLOv8 synchronous service. You will learn: - How to deploy a Node-RED instance - How to connect Node-RED to the YOLOv8 synchronous service - How to authenticate using a token - How to process image data through the workflow</p>"},{"location":"integration-node-red/#steps","title":"Steps","text":"<ol> <li> <p>Deploy YOLOv8</p> <p>Go to the <code>OSCAR dashboard</code> and select <code>Create service -&gt; FDL</code>. Use the following configuration:</p> <p>FDL: <code>yaml functions: oscar: - oscar-cluster:     name: yolov8-node-red     memory: 4Gi     cpu: '2.0'     image: ai4oshub/ai4os-yolov8-torch:latest     script: script.sh     log_level: CRITICAL</code> Script: ```bash</p> </li> <li> <p>Deploy Node-RED</p> <p>In the <code>OSCAR dashboard</code>, go to <code>Flows</code> (Sidebar panel). Enter the admin password and select or create a Bucket.</p> <p></p> <p>After deploying Node-RED we need to navigate to its user interface.</p> <p></p> <p>\u2139\ufe0f</p> <p>If you get an nginx error, you need to wait a little bit for the service to run.</p> <p></p> <p>Log in with your credentials (the user is always admin).</p> </li> <li> <p>Create a workflow in Node-RED</p> <p>Now we will create a workflow that will fetch an image from internet, make a request to Yolo8 service and visualize the result.</p> <p>We will need a the following list of components that can be found in the Node-RED sidebar menu:</p> <ul> <li>Common \u2192 <code>inject</code> node</li> <li>Network \u2192 <code>HTTP request</code> node</li> <li>Output \u2192 <code>image</code> node</li> <li>OSCAR \u2192 <code>OSCAR YOLO8</code> node</li> </ul> <p>Connect the components as shown:</p> <p></p> <p>To configure the <code>HTTP request</code> node (double-click it):</p> <ul> <li>URL: URL of an image</li> <li>Payload: Send as request body</li> <li>Return: A binary buffer</li> </ul> <p></p> <p>To configure the <code>OSCAR YOLO8</code> node (double-click it):</p> <ul> <li>Server: URL of the cluster</li> <li>Service Name: yolov8-node-red</li> <li>Token: Obtain the token from <code>OSCAR dashboard</code><code>\u2192 Info (Sidebar panel) \u2192 Access token</code></li> </ul> <p></p> </li> <li> <p>Test the workflow</p> <p>After configuring your workflow, test it in the Node-RED Editor:</p> <ol> <li>Click <code>Deploy</code> (top right corner)</li> <li>Click the <code>inject</code> node</li> </ol> <p></p> <p>You should see the result:</p> <p></p> </li> </ol>"},{"location":"integration-node-red/#binbash","title":"!/bin/bash","text":"<p>RENAMED_FILE=\"${INPUT_FILE_PATH}.png\" mv \"$INPUT_FILE_PATH\" \"$RENAMED_FILE\" OUTPUT_FILE=\"$TMP_OUTPUT_DIR/output.png\" deepaas-cli --deepaas_method_output=\"$OUTPUT_FILE\" predict --files \"$RENAMED_FILE\" --accept image/png 2&gt;&amp;1 echo \"Prediction was saved in: $OUTPUT_FILE\" ```</p>"},{"location":"integration-scone/","title":"Integration with SCONE","text":"<p>SCONE is a tool that allows confidential computing on the cloud thus protecting the data, code and application secrets on a Kubernetes cluster.  By leveraging hardware-based security features such as Intel SGX (Software Guard Extensions), SCONE ensures that sensitive data and computations remain protected even in potentially untrusted environments. This end-to-end encryption secures data both at rest and in transit, significantly reducing the risk of data breaches. Additionally, SCONE simplifies the development and deployment of secure applications by providing a seamless integration layer for existing software, thus enhancing security without requiring major code changes. </p> <p>\u26a0\ufe0f</p> <p>Please note that the usage of SCONE introduces a non-negligible overhead when executing the container for the OSCAR service.</p> <p>More info about SCONE and Kubernetes here. </p> <p>To use SCONE on a Kubernetes cluster, Intel SGX has to be enabled on the machines, and for these, the SGX Kubernetes plugin needs to be present on the cluster. Once the plugin is installed you only need to specify the parameter <code>enable_sgx</code> on the FDL of the services that are going to use a secured container image like in the following example.</p> <pre><code>functions:\n  oscar:\n  - oscar-cluster:\n      name: sgx-service\n      memory: 1Gi\n      cpu: '0.6'\n      image: your_image\n      enable_sgx: true\n      script: script.sh\n</code></pre> <p>SCONE support was introduced in OSCAR for the AI-SPRINT project to tackle the Personalized Healthcare use case in which OSCAR is employed to perform the inference phase of pre-trained models out of sensitive data captured from wearable devices. This use case was coordinated by the Barcelona Supercomputing Center (BSC) and Technische Universit\u00e4t Dresden \u2014 TU Dresden was involved for the technical activities regarding SCONE.</p>"},{"location":"invoking-async/","title":"Asynchronous invocations","text":"<p>For event-driven file processing, OSCAR automatically manages the creation and notification system of MinIO buckets. This allows the event-driven invocation of services using asynchronous requests for every file uploaded to the bucket, which generates a Kubernetes job for every file to be processed.</p> <p></p> <p>These jobs will be queued up in the Kubernetes scheduler and will be processed whenever there are resources available. If OSCAR cluster has been deployed as an elastic Kubernetes cluster (see Deployment with IM), then new Virtual Machines will be provisioned (up to the maximum number of nodes defined) in the underlying Cloud platform and seamlessly integrated into the Kubernetes clusters to proceed with the execution of jobs. These nodes will be terminated as the workload is reduced. Notice that the output files can be stored in MinIO or in any other storage back-end supported by the FaaS supervisor. </p> <p>Note that if your OSCAR service runs an AI model for inference, each job will load the AI model weights before performing the inference. You can mitigate this penalty by adjusting the inference code to process a compressed file with several images.</p> <p>If you want to process a large number of data files, consider using OSCAR Batch, a tool designed to perform batch-based processing in OSCAR clusters. It includes a coordinator tool where the user provides a MinIO bucket containing files for processing. This service calculates the optimal number of parallel service invocations that can be accommodated within the cluster, according to its current status, and distributes the image processing workload accordingly among the service invocations. This is mainly intended to process large amounts of files, for example, historical data.</p>"},{"location":"invoking-async/#log-information","title":"Log information","text":"<p>Each asynchronous invocation within OSCAR generates logs that include execution details, errors, and the service's output, which are essential for tracking job status and debugging. These logs can be accessed through the OSCAR CLI, OSCAR Dashboard or OSCAR API, allowing you to view all the jobs created for a service, as well as their status (<code>Pending</code>, <code>Running</code>, <code>Succeeded</code> or <code>Failed</code>) and their creation, start, and finish times. </p> <p>\u2139\ufe0f</p> <p>On the OSCAR Dashboard page you can find an example of asynchronous invocation and logs demonstration using the dashboard.</p>"},{"location":"invoking-sync/","title":"Synchronous invocations","text":"<p>Synchronous invocations allow obtaining the execution output as the response to the HTTP call to the <code>/run/&lt;SERVICE_NAME&gt;</code> path of the OSCAR API. For this, OSCAR delegates the execution to a serverless back-end (e.g. Knative) which uses an auto-scaled set of pods to process the requests.</p> <p>\u2139\ufe0f</p> <p>You may find references in the documentation or examples to OpenFaaS, which was used in older versions of OSCAR.  Recent versions of OSCAR use Knative as the serverless back-end for synchronous invocations, which provides several benefits such as scale-to-zero or load-balanced auto-scaled set of pods.</p> <p></p> <p>Synchronous invocations can be made through OSCAR CLI, using the command <code>oscar-cli service run</code>:</p> <pre><code>oscar-cli service run [SERVICE_NAME] {--input | --text-input} {-o | -output }\n</code></pre> <p>You can check these examples:</p> <ul> <li>plant-classification-sync</li> <li>text-to-speech.</li> </ul> <p>The input can be sent as a file via the <code>--input</code> flag, and the result of the execution will be displayed directly in the terminal:</p> <pre><code>oscar-cli service run plant-classification-sync --input images/image3.jpg\n</code></pre> <p>Alternatively, it can be sent as plain text using the <code>--text-input</code> flag and the result stored in a file using the <code>--output</code> flag:</p> <pre><code>oscar-cli service run text-to-speech --text-input \"Hello everyone\"  --output output.mp3\n</code></pre>"},{"location":"invoking-sync/#synchronous-invocations-via-oscar-cli","title":"Synchronous Invocations via OSCAR CLI","text":"<p>OSCAR CLI simplifies the execution of services synchronously via the <code>oscar-cli service run</code> command. This command requires the input to be passed as text through the <code>--text-input</code> flag or directly a file to be sent by passing its path through the <code>--input</code> flag. Both input types are automatically encoded in Base64.</p> <p>It also allows setting the <code>--output</code> flag to indicate a path for storing (and decoding if needed) the output body in a file, otherwise the output will be shown in stdout.</p> <p>An illustration of triggering a service synchronously through OSCAR-CLI can be found in the cowsay example.</p> <pre><code>oscar-cli service run cowsay --text-input '{\"message\":\"Hello World\"}'\n</code></pre>"},{"location":"invoking-sync/#synchronous-invocations-via-oscar-api","title":"Synchronous Invocations via OSCAR API","text":"<p>OSCAR services can also be invoked via traditional HTTP clients such as cURL using the path <code>/run/&lt;SERVICE_NAME&gt;</code> defined in the OSCAR API . However, you must take care to properly format the input to one of the two supported formats (JSON or Base64 encode) and include the service access token in the request.</p> <p>An illustration of triggering a service synchronously through cURL can be found in the cowsay example.</p> <p>To send an input file through cURL, you must encode it in base64 or json. To avoid issues with the output in synchronous invocations remember to put the <code>log_level</code> as <code>CRITICAL</code>. Output, which is encoded in base64 or in json, should be decoded as well. Save output in the expected format of the use-case.</p> <pre><code>base64 input.png | curl -X POST -H \"Authorization: Bearer &lt;TOKEN&gt;\" \\\n -d @- https://&lt;CLUSTER_ENDPOINT&gt;/run/&lt;OSCAR_SERVICE&gt; | base64 -d &gt; result.png\n</code></pre>"},{"location":"invoking-sync/#synchronous-invocations-via-oscar-dashboard","title":"Synchronous Invocations via OSCAR Dashboard","text":"<p>Another way to perform synchronous invocations is by using the OSCAR Dashboard, which allows you to send files in base64 format or use a built-in code editor to create payloads in a more user-friendly way. You also can access it by navigating directly to the cluster endpoint in your browser.</p> <p>After logging in, find the service you want to invoke and click its invocation button (<code>&gt;_</code>).</p> <p></p> <p>Next, you can either upload a file containing the payload or use the built-in code editor to construct it manually.</p> <p></p> <p>For demonstration purposes, we'll use the built-in code editor to send a JSON payload.</p> <p></p> <p>Finally, you can view the result of the invocation directly in the dashboard.</p> <p></p>"},{"location":"invoking-sync/#service-access-tokens","title":"Service access tokens","text":"<p>As detailed in the API specification, invocation paths require either the service access token or the Access Token of the user when the cluster is integrated with EGI Check-in, in the request header for authentication (any of them is valid). Service access tokens are auto-generated in service creation and update, and MinIO eventing system is automatically configured to use them for event-driven file processing. Tokens can be obtained through the API, using the <code>oscar-cli service get</code> command or directly from the web interface.</p> <p></p>"},{"location":"invoking-sync/#limitations","title":"Limitations","text":"<p>Although the use of the Knative Serverless Backend for synchronous invocations provides elasticity similar to the one provided by their counterparts in public clouds, such as AWS Lambda, synchronous invocations are not still the best option for run long-running resource-demanding applications, like deep learning inference or video processing. </p> <p>The synchronous invocation of long-running resource-demanding applications may lead to timeouts on Knative pods. Therefore, we consider asynchronous invocations (which generate Kubernetes jobs) as the optimal approach to handle event-driven file processing.</p>"},{"location":"invoking/","title":"Service Execution Types","text":"<p>OSCAR services can be executed:</p> <ul> <li>Synchronously, so that the invocation to the service blocks the client until the response is obtained. </li> <li>Asynchronously, typically in response to a file upload to MinIO or via the OSCAR API.</li> <li>As an exposed service, where the application executed already provides its own API or user interface (e.g. Jupyter Notebook)</li> </ul> <p>After reading the different service execution types, take into account the following considerations to better decide the most appropriate execution type for your use case:</p> <ul> <li> <p>Scalability: Asynchronous invocations provide the best throughput when dealing with multiple concurrent data processing requests, since these are processed by independent jobs which are managed by the Kubernetes scheduler. A two-level elasticity approach is used (increase in the number of pods and increase in the number of Virtual Machines, if the OSCAR cluster was configured to be elastic). This is the recommended approach when each processing request exceeds the order of tens of seconds.</p> </li> <li> <p>Reduced Latency Synchronous invocations are oriented for short-lived (&lt; tens of seconds) bursty requests. A certain number of containers can be configured to be kept alive to avoid the performance penalty of spawning new ones while providing an upper bound limit (see <code>min_scale</code> and <code>max_scale</code> in the FDL, at the expense of always consuming resources in the OSCAR cluster. If the processing file is in the order of several MBytes it may not fit in the payload of the HTTP request.</p> </li> <li> <p>Easy Access For services that provide their own user interface or their own API, exposed services provide the ability to execute them in OSCAR and benefit for an auto-scaled configuration in case they are stateless. This way, users can directly access the service using its well-known interfaces by the users. </p> </li> </ul> <p>If you want to process a large number of files, please consider using OSCAR-batch.</p>"},{"location":"license/","title":"License","text":"<pre><code>                                 Apache License\n\n                           Version 2.0, January 2004\n\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2018 GRyCAP - I3M - UPV\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n</code></pre>"},{"location":"local-testing/","title":"Local Deployment","text":"<p>\u2757\ufe0f</p> <p>The local deployment of OSCAR is just recommended for testing. Please, consider using the IM to deploy a fully-featured OSCAR cluster in a Cloud platform.</p> <p>The easiest way to test the OSCAR platform locally is using kind. Kind allows the deployment of Kubernetes clusters inside Docker containers and automatically configures <code>kubectl</code> to access them.</p>"},{"location":"local-testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker, required by kind to launch   the Kubernetes nodes on containers.</li> <li>kubectl, to   communicate with the Kubernetes cluster.</li> <li>Helm, to easily deploy applications on Kubernetes.</li> <li>Kind, to   deploy the local Kubernetes cluster.</li> </ul> <p>\u26a0\ufe0f </p> <p>Although the use of local Docker images has yet to be implemented as a feature on OSCAR clusters, the local deployment for testing allows you to use a local Docker registry to use this kind of images.  The registry uses by default the port 5001, so each image you want to use must be tagged as <code>localhost:5001/[image_name]</code> and pushed to the repository through the <code>docker push localhost:5001/[image_name]</code> command.</p> <p>Also, port 80 must be available to avoid errors during the deployment since OSCAR-Dashboard uses it. Check the Frequently Asked Questions (FAQ) for more info.</p>"},{"location":"local-testing/#automated-local-testing","title":"Automated local testing","text":"<p>To set up the enviroment for the platform testing you can run the following command. This script automatically executes all the necessary steps to deploy the local cluster and the OSCAR platform along with all the required tools. </p> <pre><code>curl -sSL http://go.oscar.grycap.net | bash\n</code></pre>"},{"location":"local-testing/#steps-for-manual-local-testing","title":"Steps for manual local testing","text":"<p>If you want to do it manualy you can follow the listed steps.</p>"},{"location":"local-testing/#create-the-cluster","title":"Create the cluster","text":"<p>To create a single node cluster with MinIO and Ingress controller ports locally accessible, run:</p> <pre><code>cat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\n  - containerPort: 30300\n    hostPort: 30300\n    protocol: TCP\n  - containerPort: 30301\n    hostPort: 30301\n    protocol: TCP\nEOF\n</code></pre>"},{"location":"local-testing/#deploy-nginx-ingress","title":"Deploy NGINX Ingress","text":"<p>To enable Ingress support for accessing the OSCAR server, we must deploy the NGINX Ingress:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/kind/deploy.yaml\n</code></pre>"},{"location":"local-testing/#deploy-minio","title":"Deploy MinIO","text":"<p>OSCAR depends on MinIO as a storage provider and function trigger. The easy way to run MinIO in a Kubernetes cluster is by installing its helm chart. To  install the helm MinIO repo and install the chart, run the following commands replacing <code>&lt;MINIO_PASSWORD&gt;</code> with a password. It must have at least 8 characters:</p> <pre><code>helm repo add minio https://charts.min.io\nhelm install minio minio/minio --namespace minio --set rootUser=minio,\\\nrootPassword=&lt;MINIO_PASSWORD&gt;,service.type=NodePort,service.nodePort=30300,\\\nconsoleService.type=NodePort,consoleService.nodePort=30301,mode=standalone,\\\nresources.requests.memory=512Mi,\\\nenvironment.MINIO_BROWSER_REDIRECT_URL=http://localhost:30301 \\\n --create-namespace\n</code></pre> <p>Note that the deployment has been configured to use the rootUser <code>minio</code> and the specified password as rootPassword. The NodePort service type has been used in order to allow access from <code>http://localhost:30300</code> (API) and <code>http://localhost:30301</code> (Console).</p>"},{"location":"local-testing/#deploy-nfs-server-provisioner","title":"Deploy NFS server provisioner","text":"<p>NFS server provisioner is required for the creation of <code>ReadWriteMany</code> PersistentVolumes in the kind cluster. This is needed by the OSCAR services to mount the volume with the FaaS Supervisor inside the job containers.</p> <p>To deploy it you can use this chart executing:</p> <pre><code>helm repo add nfs-ganesha-server-and-external-provisioner https://kubernetes-sigs.github.io/nfs-ganesha-server-and-external-provisioner/\nhelm install nfs-server-provisioner nfs-ganesha-server-and-external-provisioner/nfs-server-provisioner\n</code></pre> <p>Some Linux distributions may have problems using the NFS server provisioner with kind due to its default configuration of kernel-limit file descriptors. To workaround it, please run <code>sudo sysctl -w fs.nr_open=1048576</code>.</p>"},{"location":"local-testing/#deploy-knative-serving-as-serverless-backend-optional","title":"Deploy Knative Serving as Serverless Backend (OPTIONAL)","text":"<p>OSCAR supports Knative Serving as Serverless Backend to process synchronous invocations. If you want to deploy it in the kind cluster, first you must deploy the Knative Operator</p> <pre><code>kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.3.1/operator.yaml\n</code></pre> <p>Note that the above command deploys the version <code>v1.3.1</code> of the Operator. You can check if there are new versions here.</p> <p>Once the Operator has been successfully deployed, you can install the Knative Serving stack with the following command:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-serving\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  version: 1.3.0\n  ingress:\n    kourier:\n      enabled: true\n      service-type: ClusterIP\n  config:\n    config-features:\n      kubernetes.podspec-persistent-volume-claim: enabled\n      kubernetes.podspec-persistent-volume-write: enabled\n    network:\n      ingress-class: \"kourier.ingress.networking.knative.dev\"\nEOF\n</code></pre>"},{"location":"local-testing/#deploy-oscar","title":"Deploy OSCAR","text":"<p>First, create the <code>oscar</code> and <code>oscar-svc</code> namespaces by executing:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/grycap/oscar/master/deploy/yaml/oscar-namespaces.yaml\n</code></pre> <p>Then, add the grycap helm repo and deploy by running the following commands replacing <code>&lt;OSCAR_PASSWORD&gt;</code> with a password of your choice and <code>&lt;MINIO_PASSWORD&gt;</code> with the MinIO rootPassword, and remember to add the flag <code>--set serverlessBackend=knative</code> if you deployed it in the previous step:</p> <pre><code>helm repo add grycap https://grycap.github.io/helm-charts/\nhelm repo update \nhelm install --namespace=oscar oscar grycap/oscar \\\n --set authPass=&lt;OSCAR_PASSWORD&gt; --set service.type=ClusterIP \\\n --set ingress.create=true --set volume.storageClassName=nfs \\\n --set minIO.endpoint=http://minio.minio:9000 --set minIO.TLSVerify=false \\\n --set minIO.accessKey=minio --set minIO.secretKey=&lt;MINIO_PASSWORD&gt;\n</code></pre> <p>Now you can access to the OSCAR web interface through <code>https://localhost</code> with user <code>oscar</code> and the specified password.</p> <p>Note that the OSCAR server has been configured to use the ClusterIP service of MinIO for internal communication. This blocks the MinIO section in the OSCAR web interface, so to download and upload files you must connect directly to MinIO (<code>http://localhost:30300</code>).</p>"},{"location":"local-testing/#delete-the-cluster","title":"Delete the cluster","text":"<p>Once you have finished testing the platform, you can remove the local kind cluster by executing:</p> <pre><code>kind delete cluster\n</code></pre> <p>Remember that if you have more than one cluster created, it may be required to set the <code>--name</code> flag to specify the name of the cluster to be deleted.</p>"},{"location":"local-testing/#using-oscar-cli","title":"Using OSCAR-CLI","text":"<p>To use OSCAR-CLI in a local deployment, you should set the <code>--disable-ssl</code> flag to disable verification of the self-signed certificates:</p> <pre><code>oscar-cli cluster add oscar-cluster https://localhost oscar &lt;OSCAR_PASSWORD&gt; --disable-ssl\n</code></pre>"},{"location":"minio-bucket-replication/","title":"MinIO bucket replication","text":"<p>In scenarios where you have two linked OSCAR clusters as part of the same workflow defined in FDL, temporary network disconnections cause that data generated on the first cluster during the disconnection time is lost as well. </p> <p>To resolve this scenario we propose the use of replicated buckets on MinIO. With this approach, you can have two buckets synchronized on different OSCAR clusters so that, if the connection is lost, they will be re-synchronized when the connection is restored.</p> <p>An example of this scenario is shown on the following diagram, where there are two MinIO instances (each one on a different OSCAR cluster), and the output of the execution of service_x on the source serves as input for the service_y on the remote cluster.</p> <p></p> <p>Here is in more detail the data flow between the buckets:</p> <p>MinIO instance source</p> <ul> <li><code>input</code>: receives data and triggers the execution of OSCAR service_x.</li> <li><code>intermediate</code>: the output files from service_x are stored on this bucket and synchronized with the intermediate bucket on the remote instance. </li> </ul> <p>MinIO instance remote</p> <ul> <li><code>intermediate</code>: the synchronized bucket that stores the replicated data and triggers OSCAR service_y.</li> <li><code>output</code>: stores the output files of service_y.</li> </ul>"},{"location":"minio-bucket-replication/#considerations","title":"Considerations","text":"<p>When you create the service on the remote OSCAR cluster, the <code>intermediate</code> bucket which is both the replica and input of the OSCAR service will have the webhook event for PUT actions enabled so it can trigger the OSCAR service.</p> <p>Because, as explained below on Event handling on replication events, there are some specific events for replicated buckets, it is important to delete this event webhook to avoid getting both events every time.</p> <pre><code>mc event remove originminio/intermediate arn:aws:sqs::intermediate:webhook --event put\n</code></pre>"},{"location":"minio-bucket-replication/#helm-installation","title":"Helm installation","text":"<p>To be able to use replication each MinIO instance deployed with Helm has to be configured in distributed mode. This is done by adding the parameters <code>mode=distributed,replicas=NUM_REPLICAS</code>.</p> <p>Here is an example of a local MinIO replicated deployment with Helm:</p> <pre><code>helm install minio minio/minio --namespace minio --set rootUser=minio,rootPassword=minio123,service.type=NodePort,service.nodePort=30300,consoleService.type=NodePort,consoleService.nodePort=30301,mode=distributed,replicas=2,resources.requests.memory=512Mi,environment.MINIO_BROWSER_REDIRECT_URL=http://localhost:30301 --create-namespace\n</code></pre>"},{"location":"minio-bucket-replication/#minio-setup","title":"MinIO setup","text":"<p>To use the replication service it is necessary to set up manually both the requirements and the replication, either by command line or via the MinIO console. We created a test environment with replication via the command line as follows.</p> <p>First, we define our minIO instances (<code>originminio</code> and <code>remoteminio</code>) on the minio client.</p> <pre><code>mc alias set originminio https://localminio minioadminuser minioadminpassword\n\nmc alias set remoteminio https://remoteminio minioadminuser minioadminpassword\n</code></pre> <p>A requisite for replication is to enable the versioning on the buckets that will serve as origin and replica. When we create a service through OSCAR and the minIO buckets are created, versioning is not enabled by default, so we have to do it manually.</p> <pre><code>mc version enable originminio/intermediate\n\nmc version enable remoteminio/intermediate\n</code></pre> <p>Then, you can create the replication remote target</p> <pre><code>mc admin bucket remote add originminio/intermediate \\\n  https://RemoteUser:Password@HOSTNAME/intermediate \\\n  --service \"replication\"\n</code></pre> <p>and add the bucket replication rule so the actions on the origin bucket get synchronized on the replica.</p> <pre><code>mc replicate add originminio/intermediate \\\n   --remote-bucket 'arn:minio:replication::&lt;UUID&gt;:intermediate' \\\n   --replicate \"delete,delete-marker,existing-objects\"\n</code></pre>"},{"location":"minio-bucket-replication/#event-handling-on-replication-events","title":"Event handling on replication events","text":"<p>Once you have replica instances you can add a specific event webhook for the replica-related events.</p> <pre><code>mc event add originminio/intermediate arn:minio:sqs::intermediate:webhook --event replica\n</code></pre> <p>The replication events sometimes arrive duplicated. Although this is not yet implemented, a solution to the duplicated events would be to filter them by the <code>userMetadata</code>, which is marked as \"PENDING\" on the events to be discarded.</p> <pre><code>  \"userMetadata\": {\n    \"X-Amz-Replication-Status\": \"PENDING\"\n  }\n</code></pre> <p>MinIO documentation used</p> <ul> <li>Requirements to Set Up Bucket Replication</li> <li>Enable One-Way Server-Side Bucket Replication</li> </ul>"},{"location":"minio-usage/","title":"Using the MinIO Storage Provider","text":"<p>Each OSCAR cluster includes a deployed MinIO instance, which is used to trigger service executions. When a service is configured to use MinIO as its storage provider, it monitors a specified input folder for new data. Whenever new data is added to this folder, it triggers the associated service to execute. </p> <p>Additionally, OSCAR allows direct operations on the MinIO instance\u2014such as creating, updating, and deleting buckets\u2014that are independent of any specific service but still governed by the same visibility rules as services.</p>"},{"location":"minio-usage/#using-graphical-interfaces","title":"Using graphical interfaces","text":"<ul> <li>Using OSCAR-Dashboard: The following image highlights the section where MinIO buckets are accessible. In this section, users can view a list of buckets visible to them, including both service-associated and standalone buckets. They can also perform actions such as creating folders and uploading files.</li> </ul> <p>As shown in the image below, this section allows users to create individual buckets and define the desired visibility policy for each bucket.</p> <p></p> <ul> <li>Using the MinIO Console UI: Access details for this interface are available in the \"Info\" tab within the OSCAR Dashboard. This tab provides the MinIO console endpoint and the necessary credentials to log in, where the Access Key serves as the username, and the Secret Key functions as the password.</li> </ul> <p></p> <p>Finally, the following image provides an overview of the MinIO login panel and the \"Object Browser\" tab. Once logged in, the \"Object Browser\" tab allows users to navigate their available buckets, view stored objects, and perform various operations such as uploading, downloading, or deleting files. However, users do not have permission to create buckets directly from this interface, as each bucket requires specific policies to define its visibility.</p> <p></p> <p>Further information about the MinIO Console avaliable on MinIO Console documentation.</p>"},{"location":"minio-usage/#using-command-line-interfaces","title":"Using command-line interfaces","text":"<p>MinIO buckets can also be managed through oscar-cli command-line or the official MinIO client. </p> <ul> <li>oscar-cli: The OSCAR client provides a dedicated set of commands for accessing files within buckets. It is important to note that this interface does not support DELETE or UPDATE operations. Below is a brief overview of the available commands and their functionalities.</li> <li>get-file: Get file from a service's storage provider.</li> <li>list-files: List files from a service's storage provider path.</li> <li>put-file: Upload a file on a service storage provider.</li> </ul> <p>An example of a put-file operation:</p> <p><code>bash   oscar-cli service put-file fish-detector.yaml minio .path/to/your/images ./fish-detector/input/</code></p> <ul> <li>mc: If a user wants to use the MinIO client it needs to follow some previous steps.</li> <li>Install the client: Detailed instructions for installing the MinIO client (mc) are available in the official documentation.</li> <li> <p>Configure the MinIO instance: The client requires credentials to connect and interact with the MinIO instance. This configuration can be set with the following command:</p> <p><code>bash mc alias set myminio https://minio.gracious-varahamihira6.im.grycap.net YOUR-ACCESS-KEY YOUR-SECRET-KEY</code></p> </li> </ul> <p>Once the client is configured, users can perform various operations supported by the MinIO client. For a complete list of available commands and their usage, refer to the MinIO client reference. The following example demonstrates a PUT operation, where a file is uploaded to a specific folder within a bucket.</p> <p><code>bash   mc cp /path/to/your/images/*.jpg myminio/fish-detector/input/</code></p>"},{"location":"mount/","title":"Mounting external storage on service volumes","text":"<p>This feature enables the mounting of a folder from a storage provider, such as MinIO or dCache, into the service container. As illustrated in the following diagram, the folder is placed inside the /mnt directory on the container volume, thereby making it accessible to the service.</p> <p></p> <p>This functionality can be useful with exposed services, such as those using a Jupyter Notebook, to make the content of the storage bucket accessible directly within the Notebook. For instance, if you have a MinIO bucket called <code>notebook</code> you want to access, you would need to add the following parameters to the service definition:</p> <pre><code>mount:\n  storage_provider: minio.default\n  path: /notebook\n</code></pre> <p>The complete definition of this use case that integrates the expose of a Jupyter Notebook with the access to a mounted bucket would look like the following:</p> <pre><code>functions:\n  oscar:\n  - oscar-cluster:\n     name: jupyter\n     memory: 2Gi\n     cpu: '1.0'\n     image: jupyter/base-notebook\n     script: jupyterscript2.sh\n     environment:\n       Variables:\n         JUPYTER_TOKEN: \"root\"\n         JHUB_BASE_URL: \"/system/services/jupyter/exposed\"\n         JUPYTER_DIRECTORY: \"/mnt\"\n     mount:\n       storage_provider: minio.default\n       path: /notebook\n     expose:\n      min_scale: 1\n      max_scale: 1\n      api_port: 8888\n      cpu_threshold: 90\n      rewrite_target: true\n</code></pre> <p>Note: You can find the files of this example on OSCAR's repository examples</p> <p>As OSCAR has the credentials of the default MinIO instance internally, if you want to use a different one or a different storage provider, you need to set these credentials on the service FDL. Currently, the storage providers supported on this functionality are:</p> <ul> <li>MinIO provider</li> <li>WebDav provider</li> </ul>"},{"location":"multitenancy/","title":"Multitenancy support in OSCAR","text":"<p>In the context of OSCAR, multi-tenancy support refers to the platform's ability to enable multiple users or organizations (tenants) to deploy and run their applications or functions on the same underlying infrastructure. For this multitenancy support is required minimum version of OSCAR 3.6.0. </p> <p>In this context, services can be defined with different visibility levels: private (accessible only by the owner), public (accessible by all users), or restricted (accessible only by a specified list of users defined by the owner). This allows fine-grained control over who can access and use each deployed service.</p> <p>NOTE: By default, services are treated as private unless a visibility setting is explicitly defined.</p> <p>To use this functionality, there are some requisites that the cluster and the users have to fulfil:</p> <ul> <li> <p>The cluster needs to have enabled OIDC access.</p> <p>This is because the implementation of this functionality relies on the EGI ePUIDs to distinguish between users, therefore removing the need to manage a database.  </p> </li> <li> <p>Users who want to create restricted services need to know the ePUID of the users who will have access to the service.</p> <p>If a service's visibility is set to restricted, the service owner must provide a list of allowed users by specifying their unique user ePUID to grant them access. It\u2019s important to note that only the service creator, or owner, has permission to update the service. Allowed users can view the service and its associated buckets, and they may invoke the service, but they cannot perform any modifications or administrative actions. At creation time, the ePUID of the user creating the service doesn't need to be present on the list; however, when a service is updated, if the user who has created the service needs to access it, its ePUID must be on the list. This \"allowed users\" list is defined on the FDL at service creation time (more info in FDL docs).</p> </li> </ul> <p>NOTE: A user can obtain its EGI User ePUID by login into https://aai.egi.eu/ (for the production instance of EGI Check-In) or https://aai-demo.egi.eu (for the demo instance of EGI Check-In). </p> <p>The following is an example of an FDL that creates a service whose usage is limited to two different EGI users.</p> <pre><code>functions:\n  oscar:\n  - oscar-cluster:\n      name: grayify-multitenant\n      memory: 1Gi\n      cpu: '0.6'\n      image: ghcr.io/grycap/imagemagick\n      script: script.sh\n      vo: \"vo.example.eu\"\n      isolation_level: USER\n      visibility: restricted\n      allowed_users: \n      - \"62bb11b40398f73778b66f344d282242debb8ee3ebb106717a123ca213162926@egi.eu\"\n      - \"5e14d33ac4abc96272cc163da6a200c2e18591bfb3b0f32a4c9c867f5e938463@egi.eu\"\n      input:\n      - storage_provider: minio.default\n        path: grayify-multitenant/input\n      output:\n      - storage_provider: minio.default\n        path: grayify-multitenant/output\n</code></pre> <p>NOTE: To test the service in the FDL above, please use this script to execute the function: ImageMagick example.</p>"},{"location":"multitenancy/#isolation-level","title":"ISOLATION LEVEL","text":"<p>The isolation level variable has been added to the FDL service definition to facilitate the configuration of the service's privacy. There are 2 configurations available, SERVICE and USER. Below we explain them in more detail.</p>"},{"location":"multitenancy/#service","title":"SERVICE","text":"<p>The SERVICE isolation level is the default value. If you isolate the service at the service level and use MinIO as the storage provider (event source), the buckets indicated in the input/output sections of the service definition will be created. These buckets will only be visible to the users defined in the allowed_users list.</p>"},{"location":"multitenancy/#user","title":"USER","text":"<p>By isolating the service at the USER level, in addition to creating the buckets specified in input/output, additional private buckets will be created. Each user defined in the allowed_users list will have access to its private bucket, which will also trigger the execution of the service if a file is uploaded to the /input folder located inside it. The executions triggered by those private buckets will redirect the output into the same private bucket, in the /output folder. </p> <p>Let's see how it works with an example. Let's suppose that we are the user '5e14d33a', and we create a service called 'gray', with a default bucket also called 'gray' and a list of two users (ourselves and user '62bb11b'). Our automatically created private bucket will be 'gray-5e14d33a'. Inside that private bucket, we will find two folders: one for 'input' files and another for 'output' files. When we, as user '5e14d33a' upload a file into the 'input' folder, this provokes an event in MinIO that triggers the service. The output of this execution will be uploaded into the 'output' folder of our private bucket, meaning that the other users of the service (in the example, user '62bb11b') cannot access the inputs/outputs of our execution. THe same happens for user '62bb11b' and its private bucket. However, if we use the default 'gray' bucket, the files stored on it will be accessible by both users.</p> <p></p> <p>Since OSCAR uses MinIO as the main storage provider, so that users only have access to their designated bucket's service, MinIO users are created on-the-fly for each EGI ePUID. Consequently, each user accessing the cluster will have a MinIO user with its UID as AccessKey and an autogenerated SecretKey.</p>"},{"location":"oscar-cli/","title":"OSCAR CLI","text":"<p>OSCAR CLI provides a command line interface to interact with OSCAR. It supports cluster registrations, service management, workflows definition from FDL files and the ability to manage files from OSCAR's compatible storage providers (MinIO, AWS S3 and Onedata). The folder <code>example-workflow</code> contains all the necessary files to create a simple workflow to test the tool.</p>"},{"location":"oscar-cli/#download","title":"Download","text":""},{"location":"oscar-cli/#releases","title":"Releases","text":"<p>The easy way to download OSCAR-CLI is through the GitHub releases page. There are binaries for multiple platforms and OS. If you need a binary for another platform, please open an issue.</p>"},{"location":"oscar-cli/#install-from-source","title":"Install from source","text":"<p>If you have the Go programming language installed and configured, you can get it directly from the source by executing:</p> <pre><code>go install github.com/grycap/oscar-cli@latest\n</code></pre>"},{"location":"oscar-cli/#oidc-openid-connect","title":"OIDC (OpenID Connect)","text":"<p>If your cluster has OIDC avaliable, follow these steps to use <code>oscar-cli</code> to interact with it using the OpenID Connect.</p> <ul> <li>Install oidc-agent</li> <li>Register the EGI client</li> <li>Add a cluster in <code>oscar-cli</code> with oidc credentians (More info about the usage of the <code>cluster add</code> command here)</li> </ul> <pre><code>oscar-cli cluster add IDENTIFIER ENDPOINT --oidc-account-name SHORTNAME\n</code></pre>"},{"location":"oscar-cli/#available-commands","title":"Available commands","text":"<ul> <li>OSCAR CLI</li> <li>Download<ul> <li>Releases</li> <li>Install from source</li> <li>OIDC (OpenID Connect)</li> </ul> </li> <li>Available commands<ul> <li>apply</li> <li>cluster</li> <li>Subcommands<ul> <li>add</li> <li>default</li> <li>info</li> <li>list</li> <li>remove</li> </ul> </li> <li>service</li> <li>Subcommands of services<ul> <li>get</li> <li>list services</li> <li>remove services</li> <li>run</li> <li>logs list</li> <li>logs get</li> <li>logs remove</li> <li>get-file</li> <li>put-file</li> <li>list-files</li> </ul> </li> <li>version</li> <li>help</li> </ul> </li> </ul>"},{"location":"oscar-cli/#apply","title":"apply","text":"<p>Apply a FDL file to create or edit services in clusters.</p> <pre><code>Usage:\n  oscar-cli apply FDL_FILE [flags]\n\nAliases:\n  apply, a\n\nFlags:\n      --config string   set the location of the config file (YAML or JSON)\n  -h, --help            help for apply\n</code></pre>"},{"location":"oscar-cli/#cluster","title":"cluster","text":"<p>Manages the configuration of clusters.</p>"},{"location":"oscar-cli/#subcommands","title":"Subcommands","text":""},{"location":"oscar-cli/#add","title":"add","text":"<p>Add a new existing cluster to oscar-cli.</p> <pre><code>Usage:\n  oscar-cli cluster add IDENTIFIER ENDPOINT {USERNAME {PASSWORD | \\\n  --password-stdin} | --oidc-account-name ACCOUNT} [flags]\n\nAliases:\n  add, a\n\nFlags:\n      --disable-ssl               disable verification of ssl certificates for the\n                                  added cluster\n  -h, --help                      help for add\n  -o, --oidc-account-name string  OIDC account name to authenticate using\n                                  oidc-agent. Note that oidc-agent must be\n                                  started and properly configured\n                                  (See:https://indigo-dc.gitbook.io/oidc-agent/)\n  -t, --oidc-refresh-token string OIDC token to authenticate using oidc-token. \n                                  Note that oidc-token must be started and \n                                  properly configured\n      --password-stdin            take the password from stdin\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#default","title":"default","text":"<p>Show or set the default cluster.</p> <pre><code>Usage:\n  oscar-cli cluster default [flags]\n\nAliases:\n  default, d\n\nFlags:\n  -h, --help         help for default\n  -s, --set string   set a default cluster by passing its IDENTIFIER\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#info","title":"info","text":"<p>Show information of an OSCAR cluster.</p> <pre><code>Usage:\n  oscar-cli cluster info [flags]\n\nAliases:\n  info, i\n\nFlags:\n  -c, --cluster string   set the cluster\n  -h, --help             help for info\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#list","title":"list","text":"<p>List the configured OSCAR clusters.</p> <pre><code>Usage:\n  oscar-cli cluster list [flags]\n\nAliases:\n  list, ls\n\nFlags:\n  -h, --help   help for list\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#remove","title":"remove","text":"<p>Remove a cluster from the configuration file.</p> <pre><code>Usage:\n  oscar-cli cluster remove IDENTIFIER [flags]\n\nAliases:\n  remove, rm\n\nFlags:\n  -h, --help   help for remove\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#service","title":"service","text":"<p>Manages the services within a cluster.</p>"},{"location":"oscar-cli/#subcommands-of-services","title":"Subcommands of services","text":""},{"location":"oscar-cli/#get","title":"get","text":"<p>Get the definition of a service.</p> <pre><code>Usage:\n  oscar-cli service get SERVICE_NAME [flags]\n\nAliases:\n  get, g\n\nFlags:\n  -c, --cluster string   set the cluster\n  -h, --help             help for get\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#list-services","title":"list services","text":"<p>List the available services in a cluster.</p> <pre><code>Usage:\n  oscar-cli service list [flags]\n\nAliases:\n  list, ls\n\nFlags:\n  -c, --cluster string   set the cluster\n  -h, --help             help for list\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#remove-services","title":"remove services","text":"<p>Remove a service from the cluster.</p> <pre><code>Usage:\n  oscar-cli service remove SERVICE_NAME... [flags]\n\nAliases:\n  remove, rm\n\nFlags:\n  -c, --cluster string   set the cluster\n  -h, --help             help for remove\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#run","title":"run","text":"<p>Invoke a service synchronously (a Serverless backend in the cluster is required).</p> <pre><code>Usage:\n  oscar-cli service run SERVICE_NAME {--file-input | --text-input} [flags]\n\nAliases:\n  run, invoke, r\n\nFlags:\n  -c, --cluster string      set the cluster\n  -e, --endpoint string     endpoint of a non registered cluster\n  -f, --file-input string   input file for the request\n  -h, --help                help for run\n  -o, --output string       file path to store the output\n  -i, --text-input string   text input string for the request\n  -t, --token string        token of the service\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#logs-list","title":"logs list","text":"<p>List the logs from a service.</p> <pre><code>Usage:\n  oscar-cli service logs list SERVICE_NAME [flags]\n\nAliases:\n  list, ls\n\nFlags:\n  -h, --help             help for list\n  -s, --status strings   filter by status (Pending, Running, Succeeded or\n                         Failed), multiple values can be specified by a\n                         comma-separated string\n\nGlobal Flags:\n  -c, --cluster string   set the cluster\n      --config string    set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#logs-get","title":"logs get","text":"<p>Get the logs from a service's job.</p> <pre><code>Usage:\n  oscar-cli service logs get SERVICE_NAME JOB_NAME [flags]\n\nAliases:\n  get, g\n\nFlags:\n  -h, --help              help for get\n  -t, --show-timestamps   show timestamps in the logs\n\nGlobal Flags:\n  -c, --cluster string   set the cluster\n      --config string    set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#logs-remove","title":"logs remove","text":"<p>Remove a service's job along with its logs.</p> <pre><code>Usage:\n  oscar-cli service logs remove SERVICE_NAME \\\n   {JOB_NAME... | --succeeded | --all} [flags]\n\nAliases:\n  remove, rm\n\nFlags:\n  -a, --all         remove all logs from the service\n  -h, --help        help for remove\n  -s, --succeeded   remove succeeded logs from the service\n\nGlobal Flags:\n  -c, --cluster string   set the cluster\n      --config string    set the location of the config file (YAML or JSON)\n</code></pre> <p>Note The following subcommands will not work with MinIO if you use a local deployment due to DNS resolutions, so if you want to use a command line put/get/list files from your buckets, you can use the MinIO client command line.  Once you have the client installed you can define the cluster with the <code>mc alias</code> command like it follows: <code>mc alias set myminio https://localhost:30000 minioadminuser minioadminpassword</code> So, instead of the next subcommands, you would use: - <code>mc cp</code> to put/get files fron a bucket.  - <code>mc ls</code> to list files from a bucket.</p>"},{"location":"oscar-cli/#get-file","title":"get-file","text":"<p>Get a file from a service's storage provider.</p> <p>The STORAGE_PROVIDER argument follows the format STORAGE_PROVIDER_TYPE.STORAGE_PROVIDER_NAME, being the STORAGE_PROVIDER_TYPE one of the three supported storage providers (MinIO, S3 or Onedata) and the STORAGE_PROVIDER_NAME is the identifier for the provider set in the service's definition.</p> <pre><code>Usage:\n  oscar-cli service get-file SERVICE_NAME STORAGE_PROVIDER REMOTE_FILE \\\n   LOCAL_FILE [flags]\n\nAliases:\n  get-file, gf\n\nFlags:\n  -c, --cluster string   set the cluster\n  -h, --help             help for get-file\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#put-file","title":"put-file","text":"<p>Put a file in a service's storage provider.</p> <p>The STORAGE_PROVIDER argument follows the format STORAGE_PROVIDER_TYPE.STORAGE_PROVIDER_NAME, being the STORAGE_PROVIDER_TYPE one of the three supported storage providers (MinIO, S3 or Onedata) and the STORAGE_PROVIDER_NAME is the identifier for the provider set in the service's definition.</p> <p>NOTE: This command can not be used in a local testing deployment.</p> <pre><code>Usage:\n  oscar-cli service put-file SERVICE_NAME STORAGE_PROVIDER LOCAL_FILE \\\n   REMOTE_FILE [flags]\n\nAliases:\n  put-file, pf\n\nFlags:\n  -c, --cluster string   set the cluster\n  -h, --help             help for put-file\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#list-files","title":"list-files","text":"<p>List files from a service's storage provider path.</p> <p>The STORAGE_PROVIDER argument follows the format STORAGE_PROVIDER_TYPE.STORAGE_PROVIDER_NAME, being the STORAGE_PROVIDER_TYPE one of the three supported storage providers (MinIO, S3 or Onedata) and the STORAGE_PROVIDER_NAME is the identifier for the provider set in the service's definition.</p> <pre><code>Usage:\n  oscar-cli service list-files SERVICE_NAME STORAGE_PROVIDER REMOTE_PATH [flags]\n\nAliases:\n  list-files, list-file, lsf\n\nFlags:\n  -c, --cluster string   set the cluster\n  -h, --help             help for list-files\n\nGlobal Flags:\n      --config string   set the location of the config file (YAML or JSON)\n</code></pre>"},{"location":"oscar-cli/#version","title":"version","text":"<p>Print the version.</p> <pre><code>Usage:\n  oscar-cli version [flags]\n\nAliases:\n  version, v\n\nFlags:\n  -h, --help   help for version\n</code></pre>"},{"location":"oscar-cli/#help","title":"help","text":"<p>Help provides help for any command in the application. Simply type oscar-cli help [path to command] for full details.</p> <pre><code>Usage:\n  oscar-cli help [command] [flags]\n\nFlags:\n  -h, --help   help for help\n</code></pre>"},{"location":"oscar-service/","title":"OSCAR Service","text":"<p>OSCAR allows the creation of serverless file-processing services based on container images. These services require a user-defined script with the commands responsible of the processing. The platform automatically mounts a volume on the containers with the FaaS Supervisor component, which is in charge of:</p> <ul> <li>Downloading the file that invokes the service and make it accessible through     the <code>INPUT_FILE_PATH</code> environment variable.</li> <li>Execute the user-defined script.</li> <li>Upload the content of the output folder accessible via the <code>TMP_OUTPUT_DIR</code>     environment variable.</li> </ul>"},{"location":"oscar-service/#faas-supervisor","title":"FaaS Supervisor","text":"<p>FaaS Supervisor, the component in charge of managing the input and output of services, allows JSON or base64 encoded body in service requests. The body of these requests will be automatically decoded into the invocation's input file available from the script through the <code>$INPUT_FILE_PATH</code> environment variable.</p> <p>The output of synchronous invocations will depend on the application itself:</p> <ol> <li>If the script generates a file inside the output dir available through the     <code>$TMP_OUTPUT_DIR</code> environment variable, the result will be the file encoded in     base64.</li> <li>If the script generates more than one file inside <code>$TMP_OUTPUT_DIR</code>, the     result will be a zip archive containing all files encoded in base64.</li> <li>If there are no files in <code>$TMP_OUTPUT_DIR</code>, FaaS Supervisor will return its     logs, including the stdout of the user script run.     To avoid FaaS Supervisor's logs, you must set the service's <code>log_level</code>     to <code>CRITICAL</code>.</li> </ol> <p>This way users can adapt OSCAR's services to their own needs.</p> <p>The FaaS Supervisor supports the following storage back-ends:</p> <ul> <li>MinIO</li> <li>Amazon S3</li> <li>Webdav (and, therefore, dCache)</li> <li>Onedata (and, therefore, EGI DataHub)</li> </ul>"},{"location":"oscar-service/#container-images","title":"Container images","text":"<p>Container images on asynchronous services use the tag <code>imagePullPolicy: Always</code>, which means that Kubernetes will check for the image digest on the image registry and download it if it is not present. So, if you are using an image without a specific tag or with the latest tag, the service will automatically download and use the most recent version of the image on its executions, whenever the image is updated.</p> <p>You can follow one of the examples in order to test the OSCAR framework for specific applications. We recommend you to start with the plant classification example.</p>"},{"location":"oscar-service/#external-storage-providers","title":"External Storage Providers","text":"<p>FaaS Supervisor, download the input file and upload the output file. There are multiple Storage available to interact with. Each Storage has parameters, an authentication process, and particularities.</p>"},{"location":"oscar-service/#minio","title":"MinIO","text":"<p>OSCAR installs a MinIO inside the cluster. This default MinIO can be referenced as <code>minio</code> or <code>minio.default</code> in the input and output parameters of the OSCAR service. Suppose you define a new MinIO cluster in the storage provider. You can use a MinIO outside the cluster. MinIO Storage works as input or output without any other third-party software. Also, MinIO can make a bucket replication between two buckets.</p>"},{"location":"oscar-service/#dcache","title":"dCache","text":"<p>dCache use WebDAV protocol. dCache works as an output storage without any other third-party software. However, DCNiOS is necessary for input storage.</p> <p>The EGI check-in authentication process is not available.</p>"},{"location":"oscar-service/#s3","title":"S3","text":"<p>You have to pass the AWS credentials to the FaaS Supervisor by defining them in the environment variables with the keys <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_DEFAULT_REGION</code>. S3 works as an output storage system without any third-party software. But DCNiOS is necessary as input storage.</p>"},{"location":"oscar-service/#onedata","title":"ONEDATA","text":"<p>ONEDATA is a global data access solution for science that only works as an output storage provider. Check the ONEDATA Storage provider for more information.</p> <p></p>"},{"location":"training/","title":"Presentations and Webinars","text":""},{"location":"training/#deploy-your-ai-based-service-for-inference-using-oscar","title":"Deploy your AI-based service for inference using OSCAR","text":"<p>Delivered for the AI4EOSC and iMagine projects in March 2024.</p> <p>Deploy the YOLO model to trigger synchronous inferences in OSCAR in May 2025.</p> <p>AI4EOSC Webinar. Advancing AI Workflows and Inference with DevOps, MLOps, OSCAR and AI4Compose in April 2025</p>"},{"location":"usage-dashboard/","title":"OSCAR Dashboard","text":"<p>\u2757\ufe0f</p> <p>For simple OSCAR services you may use the Dashboard, but its features may not be on par with the latest changes in the FDL.  Therefore, it is recommended to use OSCAR CLI to deploy an OSCAR service.  </p> <p>This section details the usage of the OSCAR Dashboard with the plant classification example, from the  OSCAR examples. </p>"},{"location":"usage-dashboard/#login","title":"Login","text":"<p>OSCAR Dashboard is exposed via a Kubernetes ingress and it is accessible via the Kubernetes master node IP. </p> <p></p> <p>After a correct login, you should see the main view:</p> <p></p>"},{"location":"usage-dashboard/#deploying-services","title":"Deploying services","text":"<p>In order to create a new service, you must click on the \"Create service\" button and follow the wizard. For an OSCAR Service a script must be provided for the processing of files. This script must use the environment variables <code>INPUT_FILE_PATH</code> and <code>TMP_OUTPUT_DIR</code> to refer to the input file and the folder where to save the results respectively:</p> <pre><code>#!/bin/bash\n\necho \"SCRIPT: Invoked classify_image.py. File available in $INPUT_FILE_PATH\"\nFILE_NAME=`basename \"$INPUT_FILE_PATH\"`\nOUTPUT_FILE=\"$TMP_OUTPUT_DIR/$FILE_NAME\"\npython2 /opt/plant-classification-theano/classify_image.py \\\n \"$INPUT_FILE_PATH\" -o \"$OUTPUT_FILE\"\n</code></pre> <p>You must fill in the fields indicating the container image to use, the name of the service and the script file. In addition, you can add environment variables, specify the resources (RAM and CPUs) and choose the log level of the service.</p> <p>Note that specifying a tag in the container image used can be convenient to avoid problems with quotas for certain container registries such as Docker Hub. This is due to the fact that Kubernetes defaults the <code>imagePullPolicy</code> of pods to <code>Always</code> when no tag or the <code>latest</code> tag is set, which checks the version of the image in the registry every time a job is launched.</p> <p></p> <p>Next, we need to specify input and output storage parameters. You can utilize one or multiple storage providers supported by the platform, like MinIO, Onedata, and Amazon S3.</p> <p>As we use the service creation form provided by the web interface to create the service, we will only have access to the default MinIO storage provided with the platform. In order to configure a more complicated workflow using multiple storage providers, you need to use an FDL file to define the service. </p> <p>In this step, you must first choose the paths of the storage provider to be used as source of events, i.e. the input bucket and/or folder that will trigger the service.</p> <p>Only the <code>minio.default</code> provider can be used as input storage provider.</p> <p>After filling in each path, remember to click on the \"Save\" button.</p> <p></p> <p>Finally, the same must be done to indicate the output paths to be used in the desired storage providers. You can also indicate suffixes and/or prefixes to filter the files uploaded to each path by name.</p> <p></p> <p>\u2139\ufe0f</p> <p>Note that the resulting files can be stored in several storage providers other than MinIO, but in order to do that, you must create the service through the FDL file.</p> <p></p> <p>After clicking the \"Create\" button we will see the new service edit menu.</p> <p></p> <p>Also, we will be able to see it on the main view after a few seconds.</p> <p></p>"},{"location":"usage-dashboard/#triggering-the-service","title":"Triggering the service","text":""},{"location":"usage-dashboard/#http-endpoints","title":"HTTP endpoints","text":"<p>OSCAR services can be invoked through auto-generated HTTP endpoints. Requests to these endpoints can be made in two ways:</p> <ul> <li>Synchronous through the path <code>/run/&lt;SERVICE_NAME&gt;</code>. This redirects the     request to the OpenFaaS gateway in order to perform the processing.</li> <li>Asynchronous through the path <code>/job/&lt;SERVICE_NAME&gt;</code>. This mode is used     to perform file-processing when files are uploaded to the input storage     provider, creating a Kubernetes job per service invocation.</li> </ul> <p>The content of the HTTP request body will be stored as a file that will be available via the <code>INPUT_FILE_PATH</code> environment variable to process it.</p> <p>A detailed specification of the OSCAR's API and its different paths can be found here.</p>"},{"location":"usage-dashboard/#minio-storage-tab","title":"MinIO Storage Tab","text":"<p>MinIO Storage Tab is made to manage buckets without using MinIO UI. It simplifies the process. From MinIO Storage Tab, buckets can be created or removed and folders inside them. Furthermore, files can be uploaded to the buckets and downloaded from them. Each time a service is created or submitted an edit, the buckets that are not created will be formed.</p>"},{"location":"usage-dashboard/#uploading-files","title":"Uploading files","text":"<p>Once a service has been created, it can be invoked by uploading files to its input bucket/folder. This can be done through the MinIO web interface (accessible from the Kubernetes frontend IP, on port <code>30300</code>) or from the \"Buckets\" section in the side menu of the OSCAR web interface. Selecting that section will list the buckets created and, by clicking on their name, you will be able to see their content, upload and download files.</p> <p></p> <p>To upload files, you can simply drag and drop them into the folder in the bucket or through the upload file button.</p> <p></p> <p>The file will be uploaded, raising an event that will trigger the service.</p> <p></p> <p>Note that the web interface includes a preview button for some file formats, such as images.</p> <p></p>"},{"location":"usage-dashboard/#service-status-and-logs","title":"Service status and logs","text":"<p>When files are being processed by a service, it is important to know their status, as well as to observe the execution logs for testing. For this purpose, OSCAR includes a log view, accessible by clicking on the \"Logs\" button in a service from the main view.</p> <p></p> <p>In this view you can see all the jobs created for a service, as well as their status (\"Pending\", \"Running\", \"Succeeded\" or \"Failed\") and their creation, start and finish time.</p> <p></p> <p>To view the logs generated by a job, simply click on the drop-down button located on the right.</p> <p></p> <p>The view also features options to delete one or all logs.</p>"},{"location":"usage-dashboard/#downloading-files-from-minio","title":"Downloading files from MinIO","text":"<p>Downloading files from the platform's MinIO storage provider can also be done using the OSCAR web interface. To do it, simply select one or more files and click on the button \"Download\" or click on download icon to download one file.</p> <p></p> <p>In the following picture you can see the preview of the resulting file after the execution triggered in the previous step.</p> <p></p>"},{"location":"usage-dashboard/#deleting-services","title":"Deleting services","text":"<p>Services can be deleted by clicking on the trash can icon from the main view.</p> <p></p> <p>Once you have accepted the message shown in the image above, the service will be deleted after a few seconds.</p> <p></p>"}]}